{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Myanmar News Scraping Pipeline Walkthrough\n",
    "\n",
    "This notebook explains the web scraping component of our Myanmar news classification system. We collect articles from three news sources with different political leanings to create a balanced dataset.\n",
    "\n",
    "## Environment Setup\n",
    "**Conda Environment:** nlp  \n",
    "**Purpose:** Collect Myanmar news articles for sentiment classification training\n",
    "\n",
    "## Overview\n",
    "Our scraping system targets three Myanmar news websites:\n",
    "- **DVB News** (Democratic Voice of Burma) - Neutral/Opposition perspective\n",
    "- **Myawady News** - Government-aligned perspective  \n",
    "- **Khitthit News** - Independent/Critical perspective\n",
    "\n",
    "This creates a diverse dataset representing different political viewpoints in Myanmar media."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Core Scraping Architecture\n",
    "\n",
    "### Base Scraper Class Design\n",
    "Our scraping system uses a modular approach with a base scraper class that each news source extends:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "class BaseScraper:\n",
    "    \"\"\"\n",
    "    Base scraper class providing common functionality for all news sources.\n",
    "    \n",
    "    Key Design Principles:\n",
    "    - Respectful scraping with delays\n",
    "    - Error handling for network issues\n",
    "    - Unicode text cleaning for Myanmar content\n",
    "    - Structured data output in JSON format\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_url, delay=1.0):\n",
    "        \"\"\"\n",
    "        Initialize scraper with base configuration.\n",
    "        \n",
    "        Args:\n",
    "            base_url (str): Base URL of the news website\n",
    "            delay (float): Delay between requests in seconds\n",
    "        \"\"\"\n",
    "        self.base_url = base_url\n",
    "        self.delay = delay\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (compatible; NewsResearchBot/1.0)'\n",
    "        })\n",
    "    \n",
    "    def clean_myanmar_text(self, text):\n",
    "        \"\"\"\n",
    "        Clean Myanmar text content.\n",
    "        \n",
    "        Why this is needed:\n",
    "        - Myanmar websites often have encoding issues\n",
    "        - Mixed Unicode ranges (Myanmar + English + symbols)\n",
    "        - Extra whitespace and formatting artifacts\n",
    "        \n",
    "        Returns:\n",
    "            str: Cleaned text suitable for NLP processing\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove HTML entities and extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        # Remove special characters but preserve Myanmar script\n",
    "        # Myanmar Unicode range: U+1000â€“U+109F\n",
    "        text = re.sub(r'[^\\u1000-\\u109F\\u0020-\\u007E\\s]', ' ', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def scrape_article_list(self, max_articles=100):\n",
    "        \"\"\"\n",
    "        Get list of article URLs from the main page.\n",
    "        Each subclass implements this based on site structure.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
    "    \n",
    "    def scrape_article_content(self, article_url):\n",
    "        \"\"\"\n",
    "        Extract content from individual article page.\n",
    "        Each subclass implements this based on site structure.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
    "\n",
    "print(\"âœ… Base scraper architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DVB News Scraper Implementation\n",
    "\n",
    "DVB News provides neutral/opposition perspective content. Key challenges:\n",
    "- Dynamic content loading\n",
    "- Mixed language content (Myanmar + English)\n",
    "- Anti-bot measures requiring careful request handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DVBNewsScraper(BaseScraper):\n",
    "    \"\"\"\n",
    "    DVB News scraper implementation.\n",
    "    \n",
    "    Website characteristics:\n",
    "    - WordPress-based structure\n",
    "    - Articles in both Myanmar and English\n",
    "    - Pagination-based article listing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__('https://www.dvb.no/', delay=2.0)  # Slower to be respectful\n",
    "        self.article_selectors = {\n",
    "            'title': 'h1.entry-title',\n",
    "            'content': 'div.entry-content p',\n",
    "            'date': 'time.entry-date'\n",
    "        }\n",
    "    \n",
    "    def extract_article_links(self, soup):\n",
    "        \"\"\"\n",
    "        Extract article links from DVB main page.\n",
    "        \n",
    "        Strategy:\n",
    "        - Target article title links\n",
    "        - Filter Myanmar language articles\n",
    "        - Exclude non-news content (ads, categories)\n",
    "        \n",
    "        Returns:\n",
    "            list: URLs of Myanmar articles\n",
    "        \"\"\"\n",
    "        article_links = []\n",
    "        \n",
    "        # Find all article links\n",
    "        links = soup.find_all('a', href=True)\n",
    "        \n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            title = link.get_text(strip=True)\n",
    "            \n",
    "            # Check if this looks like a Myanmar article\n",
    "            if self.is_myanmar_article(href, title):\n",
    "                full_url = href if href.startswith('http') else self.base_url + href\n",
    "                article_links.append(full_url)\n",
    "        \n",
    "        return list(set(article_links))  # Remove duplicates\n",
    "    \n",
    "    def is_myanmar_article(self, url, title):\n",
    "        \"\"\"\n",
    "        Determine if article is Myanmar content.\n",
    "        \n",
    "        Logic:\n",
    "        - Check for Myanmar Unicode characters in title\n",
    "        - Exclude category/tag pages\n",
    "        - Exclude admin/wp-content URLs\n",
    "        \"\"\"\n",
    "        # Skip non-article URLs\n",
    "        skip_patterns = ['/category/', '/tag/', '/wp-', '/admin/', '/?']\n",
    "        if any(pattern in url for pattern in skip_patterns):\n",
    "            return False\n",
    "        \n",
    "        # Check for Myanmar script in title\n",
    "        myanmar_pattern = r'[\\u1000-\\u109F]'\n",
    "        return bool(re.search(myanmar_pattern, title))\n",
    "    \n",
    "    def scrape_article_content(self, article_url):\n",
    "        \"\"\"\n",
    "        Extract full content from DVB article page.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Article data with title, content, metadata\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.session.get(article_url, timeout=30)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Extract article components\n",
    "            title = self.extract_title(soup)\n",
    "            content = self.extract_content(soup)\n",
    "            \n",
    "            # Basic validation\n",
    "            if not title or len(content) < 50:\n",
    "                return None\n",
    "            \n",
    "            return {\n",
    "                'url': article_url,\n",
    "                'title': self.clean_myanmar_text(title),\n",
    "                'content': self.clean_myanmar_text(content),\n",
    "                'scraped_at': datetime.now().isoformat(),\n",
    "                'source': 'dvb'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {article_url}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_title(self, soup):\n",
    "        \"\"\"Extract article title using CSS selectors.\"\"\"\n",
    "        title_elem = soup.select_one(self.article_selectors['title'])\n",
    "        return title_elem.get_text(strip=True) if title_elem else \"\"\n",
    "    \n",
    "    def extract_content(self, soup):\n",
    "        \"\"\"Extract article content paragraphs.\"\"\"\n",
    "        content_elems = soup.select(self.article_selectors['content'])\n",
    "        paragraphs = [elem.get_text(strip=True) for elem in content_elems]\n",
    "        return ' '.join(paragraphs)\n",
    "\n",
    "print(\"âœ… DVB scraper implementation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Collection Strategy\n",
    "\n",
    "### Why Three Sources?\n",
    "Myanmar's media landscape requires diverse data collection:\n",
    "\n",
    "1. **DVB** - Opposition/exile media perspective\n",
    "2. **Myawady** - State-controlled media perspective\n",
    "3. **Khitthit** - Independent critical journalism\n",
    "\n",
    "This ensures our model learns to classify across the political spectrum rather than just technical writing differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_balanced_dataset(articles_per_source=100):\n",
    "    \"\"\"\n",
    "    Collect balanced dataset from all three sources.\n",
    "    \n",
    "    Strategy:\n",
    "    - Equal articles from each source\n",
    "    - Quality filtering (length, language detection)\n",
    "    - Metadata preservation for later labeling\n",
    "    \n",
    "    Args:\n",
    "        articles_per_source (int): Number of articles to collect per source\n",
    "    \n",
    "    Returns:\n",
    "        dict: Combined dataset with metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize scrapers for all sources\n",
    "    scrapers = {\n",
    "        'dvb': DVBNewsScraper(),\n",
    "        'myawady': MyAwadyNewsScraper(),  # Similar implementation\n",
    "        'khitthit': KhitthitNewsScraper()  # Similar implementation\n",
    "    }\n",
    "    \n",
    "    collected_data = {\n",
    "        'articles': [],\n",
    "        'metadata': {\n",
    "            'collection_date': datetime.now().isoformat(),\n",
    "            'sources': list(scrapers.keys()),\n",
    "            'target_per_source': articles_per_source\n",
    "        },\n",
    "        'stats': {}\n",
    "    }\n",
    "    \n",
    "    for source_name, scraper in scrapers.items():\n",
    "        print(f\"\\nðŸ” Collecting from {source_name.upper()}...\")\n",
    "        \n",
    "        # Get article URLs\n",
    "        article_urls = scraper.scrape_article_list(max_articles=articles_per_source * 2)\n",
    "        print(f\"   Found {len(article_urls)} potential articles\")\n",
    "        \n",
    "        # Scrape article content\n",
    "        articles_collected = 0\n",
    "        for i, url in enumerate(article_urls[:articles_per_source * 2]):\n",
    "            if articles_collected >= articles_per_source:\n",
    "                break\n",
    "                \n",
    "            article_data = scraper.scrape_article_content(url)\n",
    "            \n",
    "            if article_data and validate_article_quality(article_data):\n",
    "                collected_data['articles'].append(article_data)\n",
    "                articles_collected += 1\n",
    "                \n",
    "                if articles_collected % 10 == 0:\n",
    "                    print(f\"   Collected {articles_collected}/{articles_per_source}\")\n",
    "            \n",
    "            # Respectful delay\n",
    "            time.sleep(scraper.delay)\n",
    "        \n",
    "        collected_data['stats'][source_name] = articles_collected\n",
    "        print(f\"   âœ… Completed: {articles_collected} articles from {source_name}\")\n",
    "    \n",
    "    return collected_data\n",
    "\n",
    "def validate_article_quality(article_data):\n",
    "    \"\"\"\n",
    "    Quality validation for scraped articles.\n",
    "    \n",
    "    Criteria:\n",
    "    - Minimum content length (meaningful articles)\n",
    "    - Myanmar script presence (language filtering)\n",
    "    - No duplicate content\n",
    "    \n",
    "    Args:\n",
    "        article_data (dict): Article with title, content, etc.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if article meets quality standards\n",
    "    \"\"\"\n",
    "    content = article_data.get('content', '')\n",
    "    title = article_data.get('title', '')\n",
    "    \n",
    "    # Length requirements\n",
    "    if len(content) < 100:  # Minimum meaningful content\n",
    "        return False\n",
    "    \n",
    "    if len(title) < 5:  # Minimum title length\n",
    "        return False\n",
    "    \n",
    "    # Myanmar script requirement\n",
    "    myanmar_pattern = r'[\\u1000-\\u109F]'\n",
    "    if not re.search(myanmar_pattern, content + title):\n",
    "        return False\n",
    "    \n",
    "    # Content ratio check (avoid pages with mostly navigation/ads)\n",
    "    words = content.split()\n",
    "    if len(words) < 20:  # Minimum word count\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "print(\"âœ… Data collection strategy defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Output Format and Structure\n",
    "\n",
    "Our scraping system outputs structured JSON data that feeds into the next pipeline stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example output structure\n",
    "sample_scraped_data = {\n",
    "    \"articles\": [\n",
    "        {\n",
    "            \"url\": \"https://www.dvb.no/article/example-url\",\n",
    "            \"title\": \"á€™á€¼á€”á€ºá€™á€¬ á€”á€­á€¯á€„á€ºá€„á€¶á€á€±á€¬á€º á€›á€±á€¸á€›á€¬ á€á€±á€«á€„á€ºá€¸á€…á€‰á€º\",  # Myanmar title\n",
    "            \"content\": \"á€™á€¼á€”á€ºá€™á€¬ á€˜á€¬á€žá€¬ á€–á€¼á€„á€·á€º á€›á€±á€¸á€žá€¬á€¸ á€‘á€¬á€¸ á€žá€±á€¬ á€†á€±á€¬á€„á€ºá€¸á€•á€«á€¸ á€¡á€€á€¼á€±á€¬á€„á€ºá€¸á€¡á€›á€¬...\",  # Myanmar content\n",
    "            \"scraped_at\": \"2025-08-23T10:30:00\",\n",
    "            \"source\": \"dvb\",\n",
    "            \"word_count\": 245,\n",
    "            \"language_detected\": \"my\"\n",
    "        }\n",
    "    ],\n",
    "    \"metadata\": {\n",
    "        \"collection_date\": \"2025-08-23T10:30:00\",\n",
    "        \"total_articles\": 300,\n",
    "        \"sources\": [\"dvb\", \"myawady\", \"khitthit\"],\n",
    "        \"collection_stats\": {\n",
    "            \"dvb\": {\"attempted\": 150, \"successful\": 100, \"success_rate\": 0.67},\n",
    "            \"myawady\": {\"attempted\": 120, \"successful\": 100, \"success_rate\": 0.83},\n",
    "            \"khitthit\": {\"attempted\": 130, \"successful\": 100, \"success_rate\": 0.77}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def save_scraped_data(data, output_dir):\n",
    "    \"\"\"\n",
    "    Save scraped data in multiple formats for pipeline consumption.\n",
    "    \n",
    "    Outputs:\n",
    "    1. Raw JSON - Complete data for debugging\n",
    "    2. Training text - Clean text for model training\n",
    "    3. Statistics - Collection metrics\n",
    "    \n",
    "    Args:\n",
    "        data (dict): Scraped article data\n",
    "        output_dir (str): Directory to save files\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # 1. Save complete raw data\n",
    "    raw_file = f\"{output_dir}/raw_articles_{timestamp}.json\"\n",
    "    with open(raw_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # 2. Save training-ready text\n",
    "    training_file = f\"{output_dir}/training_text_{timestamp}.txt\"\n",
    "    with open(training_file, 'w', encoding='utf-8') as f:\n",
    "        for article in data['articles']:\n",
    "            # Format: title + content for each article\n",
    "            text = f\"{article['title']} {article['content']}\"\n",
    "            f.write(text + '\\n')\n",
    "    \n",
    "    # 3. Save collection statistics\n",
    "    stats_file = f\"{output_dir}/stats_{timestamp}.json\"\n",
    "    with open(stats_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data['metadata'], f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"âœ… Data saved:\")\n",
    "    print(f\"   Raw: {raw_file}\")\n",
    "    print(f\"   Training: {training_file}\")\n",
    "    print(f\"   Stats: {stats_file}\")\n",
    "\n",
    "print(\"âœ… Output format structure defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Implementation Decisions\n",
    "\n",
    "### Why These Choices?\n",
    "\n",
    "1. **Respectful Scraping:**\n",
    "   - 1-2 second delays between requests\n",
    "   - Proper User-Agent headers\n",
    "   - Session management for efficiency\n",
    "   - Error handling to avoid crashes\n",
    "\n",
    "2. **Myanmar Text Handling:**\n",
    "   - Unicode normalization for consistent text\n",
    "   - Myanmar script detection (U+1000â€“U+109F)\n",
    "   - Mixed language support\n",
    "   - Encoding preservation for NLP\n",
    "\n",
    "3. **Quality Control:**\n",
    "   - Minimum content length filtering\n",
    "   - Language detection validation\n",
    "   - Duplicate detection and removal\n",
    "   - Metadata preservation for debugging\n",
    "\n",
    "4. **Scalable Architecture:**\n",
    "   - Base class for common functionality\n",
    "   - Source-specific implementations\n",
    "   - Configurable parameters\n",
    "   - Error recovery mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Integration with Pipeline\n",
    "\n",
    "The scraping stage feeds into our data processing pipeline:\n",
    "\n",
    "```\n",
    "Scraping â†’ Cleaning â†’ Preprocessing â†’ Tokenization â†’ Labeling â†’ Training\n",
    "```\n",
    "\n",
    "**Output Files:**\n",
    "- Raw JSON files move to `data/raw/to_process/`\n",
    "- Training text files used for manual review\n",
    "- Statistics help monitor collection quality\n",
    "\n",
    "**Next Stage:** Data cleaning removes HTML artifacts, normalizes Unicode, and prepares text for NLP processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}