{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM Model Training Walkthrough\n",
    "\n",
    "This notebook explains our BiLSTM (Bidirectional Long Short-Term Memory) model training process for Myanmar news sentiment classification.\n",
    "\n",
    "## Environment Setup\n",
    "**Conda Environment:** nlp  \n",
    "**Purpose:** Train deep learning model for 3-class sentiment classification\n",
    "\n",
    "## Model Architecture Overview\n",
    "```\n",
    "Input (Myanmar Tokens) ‚Üí Embedding ‚Üí BiLSTM ‚Üí Dense ‚Üí Output (3 Classes)\n",
    "```\n",
    "\n",
    "**Classes:** \n",
    "- 0: neutral (DVB - opposition perspective)\n",
    "- 1: red (Khitthit - critical perspective)\n",
    "- 2: green (Myawady - government perspective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation for Training\n",
    "\n",
    "### Loading and Preprocessing Training Data\n",
    "Our BiLSTM model requires numerical sequences as input, so we convert Myanmar text tokens into numerical representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class TrainingDataProcessor:\n",
    "    \"\"\"\n",
    "    Prepare Myanmar text data for BiLSTM training.\n",
    "    \n",
    "    Key Functions:\n",
    "    - Convert tokens to numerical sequences\n",
    "    - Handle vocabulary management\n",
    "    - Create padded sequences for batch processing\n",
    "    - Generate training/validation splits\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_vocab_size=10000, max_sequence_length=500):\n",
    "        \"\"\"\n",
    "        Initialize data processor.\n",
    "        \n",
    "        Args:\n",
    "            max_vocab_size (int): Maximum vocabulary size (most frequent tokens)\n",
    "            max_sequence_length (int): Maximum sequence length for padding\n",
    "        \n",
    "        Why these defaults?\n",
    "        - 10k vocab: Covers most Myanmar tokens while keeping model manageable\n",
    "        - 500 length: Accommodates full articles while controlling memory usage\n",
    "        \"\"\"\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.tokenizer = None\n",
    "        self.label_mapping = {0: 'neutral', 1: 'red', 2: 'green'}\n",
    "    \n",
    "    def load_training_data(self, csv_path):\n",
    "        \"\"\"\n",
    "        Load labeled training data from CSV.\n",
    "        \n",
    "        Expected CSV format:\n",
    "        - full_text: Complete article text (title + content)\n",
    "        - tokens: Space-separated Myanmar tokens from MyWord\n",
    "        - label_numeric: Class labels (0, 1, 2)\n",
    "        - source: Original news source\n",
    "        \n",
    "        Returns:\n",
    "            pandas.DataFrame: Loaded training data\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        print(f\"üìä Training Data Summary:\")\n",
    "        print(f\"   Total articles: {len(df)}\")\n",
    "        print(f\"   Label distribution:\")\n",
    "        for label, count in df['label_numeric'].value_counts().sort_index().items():\n",
    "            label_name = self.label_mapping[label]\n",
    "            print(f\"     {label} ({label_name}): {count} articles\")\n",
    "        \n",
    "        print(f\"\\nüìà Text Statistics:\")\n",
    "        print(f\"   Avg token count: {df['token_count'].mean():.1f}\")\n",
    "        print(f\"   Max token count: {df['token_count'].max()}\")\n",
    "        print(f\"   Min token count: {df['token_count'].min()}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_tokenizer(self, texts):\n",
    "        \"\"\"\n",
    "        Create and fit Keras tokenizer on Myanmar text.\n",
    "        \n",
    "        Args:\n",
    "            texts (list): List of tokenized text strings\n",
    "        \n",
    "        Returns:\n",
    "            Tokenizer: Fitted tokenizer for text-to-sequence conversion\n",
    "        \"\"\"\n",
    "        # Use pre-tokenized text (space-separated Myanmar tokens)\n",
    "        self.tokenizer = Tokenizer(\n",
    "            num_words=self.max_vocab_size,\n",
    "            oov_token='<UNK>',  # Out-of-vocabulary token\n",
    "            filters='',  # Don't filter anything - tokens are pre-processed\n",
    "            lower=False,  # Preserve Myanmar script case\n",
    "            split=' '  # Split on spaces (tokens already separated)\n",
    "        )\n",
    "        \n",
    "        self.tokenizer.fit_on_texts(texts)\n",
    "        \n",
    "        print(f\"\\nüî§ Tokenizer Statistics:\")\n",
    "        print(f\"   Vocabulary size: {len(self.tokenizer.word_index)}\")\n",
    "        print(f\"   Most common tokens: {list(self.tokenizer.word_counts.items())[:5]}\")\n",
    "        \n",
    "        return self.tokenizer\n",
    "    \n",
    "    def prepare_sequences(self, df):\n",
    "        \"\"\"\n",
    "        Convert text to padded numerical sequences.\n",
    "        \n",
    "        Process:\n",
    "        1. Convert tokens to sequences of integers\n",
    "        2. Pad sequences to uniform length\n",
    "        3. Convert labels to categorical format\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): Training data with tokens and labels\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (X_sequences, y_categorical)\n",
    "        \"\"\"\n",
    "        # Use tokenized text (space-separated tokens)\n",
    "        texts = df['tokens'].fillna('').astype(str).tolist()\n",
    "        \n",
    "        # Create tokenizer if not exists\n",
    "        if self.tokenizer is None:\n",
    "            self.create_tokenizer(texts)\n",
    "        \n",
    "        # Convert texts to sequences\n",
    "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "        \n",
    "        # Pad sequences to uniform length\n",
    "        X = pad_sequences(\n",
    "            sequences, \n",
    "            maxlen=self.max_sequence_length,\n",
    "            padding='post',  # Pad at the end\n",
    "            truncating='post'  # Truncate at the end if too long\n",
    "        )\n",
    "        \n",
    "        # Convert labels to categorical (one-hot encoding)\n",
    "        y = to_categorical(df['label_numeric'].values, num_classes=3)\n",
    "        \n",
    "        print(f\"\\nüìê Sequence Preparation:\")\n",
    "        print(f\"   Input shape: {X.shape}\")\n",
    "        print(f\"   Output shape: {y.shape}\")\n",
    "        print(f\"   Sequence length: {self.max_sequence_length}\")\n",
    "        print(f\"   Vocabulary size: {min(len(self.tokenizer.word_index), self.max_vocab_size)}\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def calculate_class_weights(self, y_labels):\n",
    "        \"\"\"\n",
    "        Calculate class weights for handling imbalanced data.\n",
    "        \n",
    "        Why class weights?\n",
    "        - Our dataset might have unequal representation of sources\n",
    "        - Helps model learn minority classes better\n",
    "        - Prevents bias toward majority class\n",
    "        \n",
    "        Args:\n",
    "            y_labels (array): Numeric labels (not one-hot)\n",
    "        \n",
    "        Returns:\n",
    "            dict: Class weight mapping\n",
    "        \"\"\"\n",
    "        class_weights = compute_class_weight(\n",
    "            'balanced',\n",
    "            classes=np.unique(y_labels),\n",
    "            y=y_labels\n",
    "        )\n",
    "        \n",
    "        class_weight_dict = dict(enumerate(class_weights))\n",
    "        \n",
    "        print(f\"\\n‚öñÔ∏è  Class Weights:\")\n",
    "        for class_idx, weight in class_weight_dict.items():\n",
    "            label_name = self.label_mapping[class_idx]\n",
    "            print(f\"   {class_idx} ({label_name}): {weight:.3f}\")\n",
    "        \n",
    "        return class_weight_dict\n",
    "\n",
    "print(\"‚úÖ Training data processor implementation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BiLSTM Model Architecture\n",
    "\n",
    "### Why BiLSTM for Myanmar News Classification?\n",
    "- **Sequential Nature:** News articles have temporal structure\n",
    "- **Context Understanding:** BiLSTM captures both past and future context\n",
    "- **Myanmar Language:** Handles variable-length Myanmar word sequences\n",
    "- **Sentiment Analysis:** Effective for opinion and bias detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "class BiLSTMModel:\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM model for Myanmar news classification.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Embedding Layer: Convert token IDs to dense vectors\n",
    "    2. BiLSTM Layer: Process sequences bidirectionally  \n",
    "    3. Dropout: Prevent overfitting\n",
    "    4. Dense Layer: Final classification\n",
    "    5. Softmax: 3-class probability distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim=128, lstm_units=64, max_length=500):\n",
    "        \"\"\"\n",
    "        Initialize BiLSTM model architecture.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size (int): Size of vocabulary\n",
    "            embedding_dim (int): Dimension of word embeddings\n",
    "            lstm_units (int): Number of LSTM units\n",
    "            max_length (int): Maximum sequence length\n",
    "        \n",
    "        Design Decisions:\n",
    "        - embedding_dim=128: Balance between representation and efficiency\n",
    "        - lstm_units=64: Sufficient for Myanmar text complexity\n",
    "        - Bidirectional: Captures context from both directions\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_units = lstm_units\n",
    "        self.max_length = max_length\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Construct the BiLSTM model architecture.\n",
    "        \n",
    "        Layer-by-layer explanation:\n",
    "        1. Embedding: Maps token IDs ‚Üí dense vectors (learns word representations)\n",
    "        2. BiLSTM: Processes sequences forward + backward (captures full context)\n",
    "        3. Dropout: Randomly zeroes neurons (prevents overfitting)\n",
    "        4. Dense: Fully connected layer (final decision making)\n",
    "        5. Activation: Softmax for 3-class probability\n",
    "        \n",
    "        Returns:\n",
    "            keras.Model: Compiled BiLSTM model\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            # Layer 1: Embedding (token ID ‚Üí dense vector)\n",
    "            Embedding(\n",
    "                input_dim=self.vocab_size + 1,  # +1 for padding token\n",
    "                output_dim=self.embedding_dim,\n",
    "                input_length=self.max_length,\n",
    "                mask_zero=True,  # Ignore padding tokens\n",
    "                name='embedding'\n",
    "            ),\n",
    "            \n",
    "            # Layer 2: Bidirectional LSTM (sequence processing)\n",
    "            Bidirectional(\n",
    "                LSTM(\n",
    "                    self.lstm_units,\n",
    "                    dropout=0.3,  # Input dropout\n",
    "                    recurrent_dropout=0.3,  # Recurrent dropout\n",
    "                    return_sequences=False,  # Only return final output\n",
    "                ),\n",
    "                name='bilstm'\n",
    "            ),\n",
    "            \n",
    "            # Layer 3: Dropout (regularization)\n",
    "            Dropout(0.5, name='dropout'),\n",
    "            \n",
    "            # Layer 4: Dense classification layer\n",
    "            Dense(\n",
    "                32,  # Hidden units\n",
    "                activation='relu',\n",
    "                name='dense_hidden'\n",
    "            ),\n",
    "            \n",
    "            # Layer 5: Output layer (3 classes)\n",
    "            Dense(\n",
    "                3,  # neutral, red, green\n",
    "                activation='softmax',\n",
    "                name='output'\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        # Compile model with appropriate loss and metrics\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),  # Adaptive learning rate\n",
    "            loss='categorical_crossentropy',  # Multi-class classification\n",
    "            metrics=['accuracy', 'precision', 'recall']\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        print(f\"\\nüèóÔ∏è  BiLSTM Model Architecture:\")\n",
    "        print(f\"   Vocabulary size: {self.vocab_size}\")\n",
    "        print(f\"   Embedding dimension: {self.embedding_dim}\")\n",
    "        print(f\"   LSTM units: {self.lstm_units} (√ó2 for bidirectional)\")\n",
    "        print(f\"   Max sequence length: {self.max_length}\")\n",
    "        print(f\"   Total parameters: {model.count_params():,}\")\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_model(self, X_train, y_train, X_val, y_val, \n",
    "                   class_weights=None, epochs=50, batch_size=32):\n",
    "        \"\"\"\n",
    "        Train the BiLSTM model.\n",
    "        \n",
    "        Training Strategy:\n",
    "        - Early stopping: Prevent overfitting\n",
    "        - Model checkpointing: Save best model\n",
    "        - Class weights: Handle imbalanced data\n",
    "        - Validation monitoring: Track generalization\n",
    "        \n",
    "        Args:\n",
    "            X_train, y_train: Training data\n",
    "            X_val, y_val: Validation data\n",
    "            class_weights: Dict of class weights\n",
    "            epochs: Maximum training epochs\n",
    "            batch_size: Training batch size\n",
    "        \n",
    "        Returns:\n",
    "            History: Training history for analysis\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not built. Call build_model() first.\")\n",
    "        \n",
    "        # Setup callbacks for training control\n",
    "        callbacks = [\n",
    "            # Early stopping: Stop if validation loss doesn't improve\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=10,  # Wait 10 epochs for improvement\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            \n",
    "            # Model checkpoint: Save best model\n",
    "            ModelCheckpoint(\n",
    "                'best_bilstm_model.h5',\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        print(f\"\\nüöÄ Starting BiLSTM Training:\")\n",
    "        print(f\"   Training samples: {len(X_train)}\")\n",
    "        print(f\"   Validation samples: {len(X_val)}\")\n",
    "        print(f\"   Batch size: {batch_size}\")\n",
    "        print(f\"   Max epochs: {epochs}\")\n",
    "        print(f\"   Early stopping patience: 10\")\n",
    "        \n",
    "        # Train the model\n",
    "        self.history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            class_weight=class_weights,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluate trained model performance.\n",
    "        \n",
    "        Metrics:\n",
    "        - Overall accuracy\n",
    "        - Per-class precision, recall, F1\n",
    "        - Confusion matrix\n",
    "        \n",
    "        Args:\n",
    "            X_test, y_test: Test data\n",
    "        \n",
    "        Returns:\n",
    "            dict: Evaluation results\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train_model() first.\")\n",
    "        \n",
    "        # Get predictions\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        \n",
    "        # Convert from categorical to class indices\n",
    "        y_true_labels = np.argmax(y_test, axis=1)\n",
    "        y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "        \n",
    "        # Classification report\n",
    "        target_names = ['neutral', 'red', 'green']\n",
    "        class_report = classification_report(\n",
    "            y_true_labels, y_pred_labels,\n",
    "            target_names=target_names,\n",
    "            output_dict=True\n",
    "        )\n",
    "        \n",
    "        # Confusion matrix\n",
    "        conf_matrix = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "        \n",
    "        print(f\"\\nüìä Model Evaluation Results:\")\n",
    "        print(f\"   Overall Accuracy: {class_report['accuracy']:.3f}\")\n",
    "        print(f\"   Macro F1-Score: {class_report['macro avg']['f1-score']:.3f}\")\n",
    "        print(f\"   Weighted F1-Score: {class_report['weighted avg']['f1-score']:.3f}\")\n",
    "        \n",
    "        print(f\"\\nüìà Per-Class Performance:\")\n",
    "        for i, class_name in enumerate(target_names):\n",
    "            metrics = class_report[class_name]\n",
    "            print(f\"   {class_name}: P={metrics['precision']:.3f}, \"\n",
    "                  f\"R={metrics['recall']:.3f}, F1={metrics['f1-score']:.3f}\")\n",
    "        \n",
    "        return {\n",
    "            'classification_report': class_report,\n",
    "            'confusion_matrix': conf_matrix,\n",
    "            'predictions': y_pred,\n",
    "            'true_labels': y_true_labels,\n",
    "            'pred_labels': y_pred_labels\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ BiLSTM model implementation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Process and Monitoring\n",
    "\n",
    "### Complete Training Pipeline\n",
    "Shows the end-to-end training workflow with monitoring and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_myanmar_news_classifier(csv_path, model_output_dir):\n",
    "    \"\"\"\n",
    "    Complete training pipeline for Myanmar news classification.\n",
    "    \n",
    "    Pipeline Steps:\n",
    "    1. Load and analyze training data\n",
    "    2. Prepare sequences for neural network\n",
    "    3. Create train/validation split\n",
    "    4. Build and configure BiLSTM model\n",
    "    5. Train with monitoring and callbacks\n",
    "    6. Evaluate performance\n",
    "    7. Save model and artifacts\n",
    "    \n",
    "    Args:\n",
    "        csv_path (str): Path to labeled training CSV\n",
    "        model_output_dir (str): Directory to save model and reports\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (trained_model, evaluation_results)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "    import pickle\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    print(f\"üá≤üá≤ Myanmar News Classification Training\")\n",
    "    print(f\"   Start time: {datetime.now()}\")\n",
    "    print(f\"   Data source: {csv_path}\")\n",
    "    print(f\"   Output directory: {model_output_dir}\")\n",
    "    \n",
    "    # Step 1: Load and prepare data\n",
    "    print(f\"\\nüìÇ Step 1: Loading training data...\")\n",
    "    processor = TrainingDataProcessor(max_vocab_size=10000, max_sequence_length=500)\n",
    "    df = processor.load_training_data(csv_path)\n",
    "    \n",
    "    # Step 2: Create sequences\n",
    "    print(f\"\\nüîÑ Step 2: Converting text to sequences...\")\n",
    "    X, y = processor.prepare_sequences(df)\n",
    "    \n",
    "    # Step 3: Train/validation split (stratified to maintain class balance)\n",
    "    print(f\"\\nüìä Step 3: Creating train/validation split...\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=df['label_numeric'], random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"   Training set: {len(X_train)} samples\")\n",
    "    print(f\"   Validation set: {len(X_val)} samples\")\n",
    "    \n",
    "    # Step 4: Calculate class weights for balanced training\n",
    "    class_weights = processor.calculate_class_weights(df['label_numeric'].values)\n",
    "    \n",
    "    # Step 5: Build BiLSTM model\n",
    "    print(f\"\\nüèóÔ∏è  Step 4: Building BiLSTM model...\")\n",
    "    vocab_size = min(len(processor.tokenizer.word_index), processor.max_vocab_size)\n",
    "    \n",
    "    model = BiLSTMModel(\n",
    "        vocab_size=vocab_size,\n",
    "        embedding_dim=128,\n",
    "        lstm_units=64,\n",
    "        max_length=processor.max_sequence_length\n",
    "    )\n",
    "    \n",
    "    compiled_model = model.build_model()\n",
    "    \n",
    "    # Step 6: Train the model\n",
    "    print(f\"\\nüöÄ Step 5: Training BiLSTM model...\")\n",
    "    history = model.train_model(\n",
    "        X_train, y_train,\n",
    "        X_val, y_val,\n",
    "        class_weights=class_weights,\n",
    "        epochs=50,\n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    # Step 7: Evaluate performance\n",
    "    print(f\"\\nüìä Step 6: Evaluating model performance...\")\n",
    "    evaluation_results = model.evaluate_model(X_val, y_val)\n",
    "    \n",
    "    # Step 8: Save model and artifacts\n",
    "    print(f\"\\nüíæ Step 7: Saving model and artifacts...\")\n",
    "    \n",
    "    # Save trained model\n",
    "    model_path = f\"{model_output_dir}/bilstm_model_{timestamp}.h5\"\n",
    "    compiled_model.save(model_path)\n",
    "    \n",
    "    # Save tokenizer for future predictions\n",
    "    tokenizer_path = f\"{model_output_dir}/tokenizer_{timestamp}.pickle\"\n",
    "    with open(tokenizer_path, 'wb') as f:\n",
    "        pickle.dump(processor.tokenizer, f)\n",
    "    \n",
    "    # Save model parameters\n",
    "    params_path = f\"{model_output_dir}/model_params_{timestamp}.pickle\"\n",
    "    model_params = {\n",
    "        'vocab_size': vocab_size,\n",
    "        'embedding_dim': model.embedding_dim,\n",
    "        'lstm_units': model.lstm_units,\n",
    "        'max_length': model.max_length,\n",
    "        'label_mapping': processor.label_mapping\n",
    "    }\n",
    "    with open(params_path, 'wb') as f:\n",
    "        pickle.dump(model_params, f)\n",
    "    \n",
    "    # Generate training report\n",
    "    create_training_report(\n",
    "        model_output_dir, timestamp, history, evaluation_results, \n",
    "        model_params, df, class_weights\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Training Complete!\")\n",
    "    print(f\"   Model saved: {model_path}\")\n",
    "    print(f\"   Tokenizer saved: {tokenizer_path}\")\n",
    "    print(f\"   Parameters saved: {params_path}\")\n",
    "    print(f\"   Final validation accuracy: {evaluation_results['classification_report']['accuracy']:.3f}\")\n",
    "    \n",
    "    return compiled_model, evaluation_results\n",
    "\n",
    "def create_training_report(output_dir, timestamp, history, evaluation, \n",
    "                         model_params, df, class_weights):\n",
    "    \"\"\"\n",
    "    Generate comprehensive training report with visualizations.\n",
    "    \n",
    "    Creates:\n",
    "    1. Training history plots (loss, accuracy)\n",
    "    2. Confusion matrix visualization\n",
    "    3. Detailed text report\n",
    "    4. Model architecture summary\n",
    "    \"\"\"\n",
    "    import json\n",
    "    \n",
    "    report_dir = f\"{output_dir}/training_report_{timestamp}\"\n",
    "    os.makedirs(report_dir, exist_ok=True)\n",
    "    \n",
    "    # 1. Plot training history\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Loss plot\n",
    "    ax1.plot(history.history['loss'], label='Training Loss')\n",
    "    ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax1.set_title('Model Loss During Training')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax2.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    ax2.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax2.set_title('Model Accuracy During Training')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{report_dir}/training_history.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        evaluation['confusion_matrix'],\n",
    "        annot=True, fmt='d',\n",
    "        xticklabels=['neutral', 'red', 'green'],\n",
    "        yticklabels=['neutral', 'red', 'green'],\n",
    "        cmap='Blues'\n",
    "    )\n",
    "    plt.title('Confusion Matrix - Myanmar News Classification')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{report_dir}/confusion_matrix.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Create detailed text report\n",
    "    report = {\n",
    "        'training_info': {\n",
    "            'timestamp': timestamp,\n",
    "            'model_type': 'Bidirectional LSTM',\n",
    "            'dataset_size': len(df),\n",
    "            'training_epochs': len(history.history['loss']),\n",
    "            'early_stopping': len(history.history['loss']) < 50\n",
    "        },\n",
    "        'model_architecture': model_params,\n",
    "        'class_weights': class_weights,\n",
    "        'performance_metrics': evaluation['classification_report'],\n",
    "        'final_metrics': {\n",
    "            'final_train_loss': history.history['loss'][-1],\n",
    "            'final_val_loss': history.history['val_loss'][-1],\n",
    "            'final_train_acc': history.history['accuracy'][-1],\n",
    "            'final_val_acc': history.history['val_accuracy'][-1],\n",
    "            'best_val_acc': max(history.history['val_accuracy'])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save as JSON\n",
    "    with open(f\"{report_dir}/training_report.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"   üìã Training report saved: {report_dir}/\")\n",
    "\n",
    "print(\"‚úÖ Training pipeline implementation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Performance Analysis\n",
    "\n",
    "### Key Performance Metrics\n",
    "\n",
    "**Why These Metrics Matter:**\n",
    "- **Accuracy:** Overall correctness across all classes\n",
    "- **Precision:** How many predicted positives are actually positive\n",
    "- **Recall:** How many actual positives are correctly identified\n",
    "- **F1-Score:** Harmonic mean of precision and recall\n",
    "- **Confusion Matrix:** Detailed per-class performance breakdown\n",
    "\n",
    "**Expected Performance:**\n",
    "- Target accuracy: >75% (3-class classification)\n",
    "- Balanced performance across all classes\n",
    "- Low false positive rate between similar classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_predictions(model, X_test, y_test, tokenizer, test_df):\n",
    "    \"\"\"\n",
    "    Detailed analysis of model predictions for interpretation.\n",
    "    \n",
    "    Analysis includes:\n",
    "    - Confidence distribution\n",
    "    - Misclassification patterns  \n",
    "    - Feature importance (attention weights)\n",
    "    - Sample predictions with explanations\n",
    "    \n",
    "    Args:\n",
    "        model: Trained BiLSTM model\n",
    "        X_test, y_test: Test data\n",
    "        tokenizer: Fitted tokenizer\n",
    "        test_df: Original test DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        dict: Analysis results\n",
    "    \"\"\"\n",
    "    # Get predictions with confidence scores\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred_proba, axis=1)\n",
    "    y_true_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Calculate prediction confidence\n",
    "    prediction_confidence = np.max(y_pred_proba, axis=1)\n",
    "    \n",
    "    # Identify misclassifications\n",
    "    misclassified_indices = np.where(y_pred_classes != y_true_classes)[0]\n",
    "    correct_indices = np.where(y_pred_classes == y_true_classes)[0]\n",
    "    \n",
    "    print(f\"\\nüîç Prediction Analysis:\")\n",
    "    print(f\"   Total predictions: {len(y_pred_classes)}\")\n",
    "    print(f\"   Correct predictions: {len(correct_indices)} ({len(correct_indices)/len(y_pred_classes)*100:.1f}%)\")\n",
    "    print(f\"   Misclassifications: {len(misclassified_indices)} ({len(misclassified_indices)/len(y_pred_classes)*100:.1f}%)\")\n",
    "    print(f\"   Average confidence: {np.mean(prediction_confidence):.3f}\")\n",
    "    \n",
    "    # Confidence distribution by correctness\n",
    "    correct_confidence = prediction_confidence[correct_indices]\n",
    "    incorrect_confidence = prediction_confidence[misclassified_indices]\n",
    "    \n",
    "    print(f\"\\nüìä Confidence Analysis:\")\n",
    "    print(f\"   Correct predictions confidence: {np.mean(correct_confidence):.3f} ¬± {np.std(correct_confidence):.3f}\")\n",
    "    if len(incorrect_confidence) > 0:\n",
    "        print(f\"   Incorrect predictions confidence: {np.mean(incorrect_confidence):.3f} ¬± {np.std(incorrect_confidence):.3f}\")\n",
    "    \n",
    "    # Analyze misclassification patterns\n",
    "    if len(misclassified_indices) > 0:\n",
    "        print(f\"\\n‚ùå Misclassification Patterns:\")\n",
    "        label_names = ['neutral', 'red', 'green']\n",
    "        \n",
    "        for true_class in [0, 1, 2]:\n",
    "            for pred_class in [0, 1, 2]:\n",
    "                if true_class != pred_class:\n",
    "                    pattern_count = np.sum(\n",
    "                        (y_true_classes[misclassified_indices] == true_class) &\n",
    "                        (y_pred_classes[misclassified_indices] == pred_class)\n",
    "                    )\n",
    "                    if pattern_count > 0:\n",
    "                        print(f\"   {label_names[true_class]} ‚Üí {label_names[pred_class]}: {pattern_count} cases\")\n",
    "    \n",
    "    # Sample analysis: Show best and worst predictions\n",
    "    print(f\"\\nüéØ Sample Predictions:\")\n",
    "    \n",
    "    # Best confident correct predictions\n",
    "    if len(correct_indices) > 0:\n",
    "        best_correct_idx = correct_indices[np.argmax(correct_confidence)]\n",
    "        print(f\"   Best confident correct prediction:\")\n",
    "        print(f\"     True: {['neutral', 'red', 'green'][y_true_classes[best_correct_idx]]}\")\n",
    "        print(f\"     Predicted: {['neutral', 'red', 'green'][y_pred_classes[best_correct_idx]]}\")\n",
    "        print(f\"     Confidence: {prediction_confidence[best_correct_idx]:.3f}\")\n",
    "    \n",
    "    # Worst misclassifications (high confidence but wrong)\n",
    "    if len(misclassified_indices) > 0:\n",
    "        worst_misclass_idx = misclassified_indices[np.argmax(incorrect_confidence)]\n",
    "        print(f\"   Worst misclassification (high confidence, wrong):\")\n",
    "        print(f\"     True: {['neutral', 'red', 'green'][y_true_classes[worst_misclass_idx]]}\")\n",
    "        print(f\"     Predicted: {['neutral', 'red', 'green'][y_pred_classes[worst_misclass_idx]]}\")\n",
    "        print(f\"     Confidence: {prediction_confidence[worst_misclass_idx]:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'prediction_confidence': prediction_confidence,\n",
    "        'correct_indices': correct_indices,\n",
    "        'misclassified_indices': misclassified_indices,\n",
    "        'confidence_stats': {\n",
    "            'mean_confidence': np.mean(prediction_confidence),\n",
    "            'correct_confidence': np.mean(correct_confidence) if len(correct_confidence) > 0 else 0,\n",
    "            'incorrect_confidence': np.mean(incorrect_confidence) if len(incorrect_confidence) > 0 else 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Model analysis implementation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Best Practices and Optimization\n",
    "\n",
    "### Hyperparameter Tuning Strategy\n",
    "\n",
    "**Key Parameters to Optimize:**\n",
    "1. **Embedding Dimension (64-256):** Balance between representation and efficiency\n",
    "2. **LSTM Units (32-128):** Network capacity vs. overfitting risk\n",
    "3. **Learning Rate (0.0001-0.01):** Convergence speed vs. stability\n",
    "4. **Batch Size (16-64):** Memory usage vs. gradient stability\n",
    "5. **Dropout Rate (0.2-0.6):** Regularization strength\n",
    "\n",
    "### Model Validation Strategy\n",
    "- **Stratified Split:** Maintain class balance in train/val sets\n",
    "- **Early Stopping:** Prevent overfitting on small datasets\n",
    "- **Class Weights:** Handle imbalanced Myanmar news sources\n",
    "- **Cross-Validation:** For robust performance estimation\n",
    "\n",
    "### Production Deployment Considerations\n",
    "- **Model Serialization:** Save complete model + tokenizer + parameters\n",
    "- **Inference Speed:** BiLSTM is relatively fast for classification\n",
    "- **Memory Usage:** Monitor embedding + LSTM memory requirements\n",
    "- **Model Updates:** Retrain periodically with new Myanmar news data\n",
    "\n",
    "### Expected Training Results\n",
    "- **Training Time:** 15-30 minutes on GPU (depends on dataset size)\n",
    "- **Target Accuracy:** 75-85% for 3-class Myanmar sentiment classification\n",
    "- **Convergence:** Usually within 20-30 epochs with early stopping\n",
    "- **Model Size:** ~2-5MB (embedding + LSTM weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}