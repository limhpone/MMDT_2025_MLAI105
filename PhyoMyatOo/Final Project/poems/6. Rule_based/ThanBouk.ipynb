{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "343370ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# ==============================================================================\n",
    "# The Final, Grammatically Precise Tokenizer for Clean Syllables\n",
    "# This is used in Step 2 of the main function below.\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# The Main \"Syllable Splitter\" Function\n",
    "# ==============================================================================\n",
    "def custom_syllable_splitter(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Performs a deep syllable split by breaking down all consonant stacks.\n",
    "    This function uses a two-step process to achieve the required logic for\n",
    "    cases like '·Äî·ÄÄ·Äπ·ÄÅ·Äê·Äπ·Äê' and '·Ä•·ÄÄ·Äπ·ÄÄ·Äã·Äπ·Äå'.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # --- Step 1: Pre-processing to split stacks using a loop ---\n",
    "    # This loop is the only reliable way to handle chained stacks.\n",
    "    stacked_consonant_pattern = r'([·ÄÄ-·Ä°])(·Ä∫?·Äπ)([·ÄÄ-·Ä°])'\n",
    "    processed_text = text\n",
    "    while re.search(stacked_consonant_pattern, processed_text):\n",
    "        processed_text = re.sub(stacked_consonant_pattern, r'\\1·Ä∫'  + r'\\3', processed_text)\n",
    "    processed_text = re.sub(r\"(([A-Za-z0-9]+)|[·ÄÄ-·Ä°|·Ä•|·Ä¶](·ÄÑ·Ä∫·Äπ|[·ÄÄ-·Ä°|·Ä•][·Äæ]*[·Ä∑·Ä∏]*[·Ä∫]|·Äπ[·ÄÄ-·Ä°]|[·Ä´-·Äæ·ÇèÍ©ª][Í©ª]*){0,}|.)\",r\"\\1 \", processed_text)\n",
    "    print()\n",
    "   #Step 2: Tokenization of the processed parts ---\n",
    "   # The string is now clean of stacks, so we can tokenize it reliably.\n",
    "    final_list = processed_text.split(\" \")\n",
    "    \n",
    "    # Filter out empty strings caused by trailing spaces\n",
    "    final_list = [word for word in final_list if word.strip()]\n",
    "        \n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bc13480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of all Burmese consonants (including the great '·Ä°')\n",
    "# This is used to distinguish consonants from vowels/diacritics.\n",
    "# --- Constants ---\n",
    "BURMESE_CONSONANTS = \"·ÄÄ·ÄÅ·ÄÇ·ÄÉ·ÄÑ·ÄÖ·ÄÜ·Äá·Äà·Ää·Äã·Äå·Äç·Äé·Äè·Äê·Äë·Äí·Äì·Äî·Äï·Äñ·Äó·Äò·Äô·Äö·Äõ·Äú·Äù·Äû·Äü·Ä†·Ä°\"\n",
    "# Consonantal medials (part of the onset)\n",
    "CONSONANTAL_MEDIALS = \"·Äª·Äº·Äæ·ÄΩ\" \n",
    "\n",
    "# --- The Rhyme Group Normalization Map ---\n",
    "# This is the heart of the logic. It maps an entire spelled rime\n",
    "# to its canonical phonetic group. This is robust and easy to extend.\n",
    "# Key: Orthographic (spelled) rime\n",
    "# Value: Normalized phonetic rime for comparison\n",
    "RIME_NORMALIZATION_MAP = {\n",
    "    # -at sound (·Ä°·ÄÄ·Ä∫)\n",
    "    \"·Äê·Ä∫\": \"·ÄÄ·Ä∫\", \"·Äï·Ä∫\": \"·ÄÄ·Ä∫\",  \n",
    "    # -it sound (·Ä°·Ä≠·ÄÄ·Ä∫)\n",
    "    \"·Ä≠·Äê·Ä∫\": \"·Ä≠·ÄÄ·Ä∫\", \"·Ä≠·Äï·Ä∫\": \"·Ä≠·ÄÄ·Ä∫\", \"·Ä≠·ÄÖ·Ä∫\": \"·Ä≠·ÄÄ·Ä∫\",\n",
    "    # -ut sound (·Ä°·ÄØ·Äê·Ä∫)\n",
    "    \"·ÄØ·Äê·Ä∫\": \"·ÄØ·Äï·Ä∫\",\n",
    "    # -et sound (·Ä°·ÄÄ·Ä∫) - Note: different vowel from -at, but uses same final\n",
    "    \"·ÄÄ·Ä∫\": \"·ÄÄ·Ä∫\",\n",
    "    \n",
    "    # -an sound (·Ä°·Äî·Ä∫)\n",
    "    \"·Äô·Ä∫\": \"·Äî·Ä∫\",\n",
    "    # -in sound (·Ä°·Ä≠·Äî·Ä∫)\n",
    "    \"·Ä≠·Äô·Ä∫\": \"·Ä≠·Äî·Ä∫\",\n",
    "    # -un sound (·Ä°·ÄØ·Äî·Ä∫)\n",
    "    \"·ÄØ·Äî·Ä∫\": \"·Ä≠·Äî·Ä∫\", # This can sometimes be phonetic, though less common\n",
    "    \n",
    "    # Special finals\n",
    "    \"·Äâ·Ä∫\": \"·Äî·Ä∫\", # Sounds like -an\n",
    "    \"·Ää·Ä∫\": \"·Äö·Ä∫\", # Sounds like -ay\n",
    "\n",
    "    \"·Äö·Ä∑·Ä∫\":\"·Ä≤·Ä∑\"\n",
    "}\n",
    "\n",
    "\n",
    "def _get_onset_length(word: str) -> int:\n",
    "    \"\"\"Finds the length of the initial consonant cluster (onset).\"\"\"\n",
    "    if not word or word[0] not in BURMESE_CONSONANTS:\n",
    "        return 0\n",
    "    \n",
    "    onset_len = 1\n",
    "    # Greedily consume any following consonantal medials\n",
    "    while onset_len < len(word) and word[onset_len] in CONSONANTAL_MEDIALS:\n",
    "        onset_len += 1\n",
    "    return onset_len\n",
    "\n",
    "# --- The Main Rhyme Function ---\n",
    "def get_rhyme_group(word: str) -> str:\n",
    "    \"\"\"\n",
    "    Finds the phonetically normalized rhyme part of a Burmese word.\n",
    "    \n",
    "    This function correctly isolates the rime (vowel + final) and normalizes it\n",
    "    according to Burmese poetic rules (·ÄÄ·Ä¨·Äõ·Äî·Ä∫·Äù·ÄÇ·Ä∫), ensuring both the\n",
    "    vowel and final consonant sound group are respected.\n",
    "    \"\"\"\n",
    "    # 1. Isolate the rime by stripping the onset.\n",
    "    onset_len = _get_onset_length(word)\n",
    "    rime = word[onset_len:]\n",
    "    rime = rime.replace('·Ä´', '·Ä¨')\n",
    "\n",
    "\n",
    "    if not rime:\n",
    "        return \"\"\n",
    "    \n",
    "    # 2. Normalize the rime using the map.\n",
    "    # If the rime is in our map, return its canonical value. Otherwise, return the rime itself.\n",
    "    return RIME_NORMALIZATION_MAP.get(rime, rime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6616080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "üìú Analyzing Poem:\n",
      "\n",
      "------------------------------\n",
      "‚úÖ Rule Passed: Poem has 3 lines.\n",
      "\n",
      "\n",
      "\n",
      "‚úÖ Rule Passed: Line 1 has 4 words.\n",
      "‚úÖ Rule Passed: Line 2 has 3 words.\n",
      "‚úÖ Rule Passed: Line 3 has 5 words (which is 5 or 7).\n",
      "  - Rhyme part for '·Äï·Äº': ''\n",
      "  - Rhyme part for '·ÄÄ': ''\n",
      "  - Rhyme part for '·ÄÅ·Äª': ''\n",
      "\n",
      "‚úÖ Rule Passed: The 4-3-2 rhyme scheme is correct.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def check_than_bauk(poem: str):\n",
    "    \"\"\"\n",
    "    Analyzes a given poem to check if it follows the rules of Than-Bauk.\n",
    "    \"\"\"\n",
    "    print(\"-\" * 30)\n",
    "    print(\"üìú Analyzing Poem:\\n\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # 1. Preprocessing: Split into lines and words\n",
    "    # Filter out any empty lines that might result from extra newlines\n",
    "    lines = [line.strip() for line in poem.strip().split('\\n') if line.strip()]\n",
    "\n",
    "    # --- Rule 1: Check for 3 lines ---\n",
    "    if len(lines) != 3:\n",
    "        print(f\"‚ùå Rule Failed: A Than-Bauk must have exactly 3 lines. (Found: {len(lines)})\")\n",
    "        print(\"-\" * 30)\n",
    "        return\n",
    "\n",
    "    print(\"‚úÖ Rule Passed: Poem has 3 lines.\")\n",
    "    words_by_line = [custom_syllable_splitter(line) for line in lines]\n",
    "    # --- Rule 2: Check word counts per line ---\n",
    "    all_word_counts_valid = True\n",
    "    \n",
    "    # Line 1 word count\n",
    "    if len(words_by_line[0]) == 4:\n",
    "        print(f\"‚úÖ Rule Passed: Line 1 has 4 words.\")\n",
    "    else:\n",
    "        print(f\"‚ùå Rule Failed: Line 1 must have 4 words. (Found: {len(words_by_line[0])})\")\n",
    "        all_word_counts_valid = False\n",
    "        \n",
    "    # Line 2 word count\n",
    "    if len(words_by_line[1]) == 3:\n",
    "        print(f\"‚úÖ Rule Passed: Line 2 has 3 words.\")\n",
    "    else:\n",
    "        print(f\"‚ùå Rule Failed: Line 2 must have 3 words. (Found: {len(words_by_line[1])})\")\n",
    "        all_word_counts_valid = False\n",
    "        \n",
    "    # Line 3 word count\n",
    "    if len(words_by_line[2]) in [5, 7]:\n",
    "        print(f\"‚úÖ Rule Passed: Line 3 has {len(words_by_line[2])} words (which is 5 or 7).\")\n",
    "    else:   \n",
    "        print(f\"‚ùå Rule Failed: Line 3 must have 5 or 7 words. (Found: {len(words_by_line[2])})\")\n",
    "        all_word_counts_valid = False\n",
    "        \n",
    "    # If word counts are wrong, we can't reliably check rhymes, so we stop.\n",
    "    if not all_word_counts_valid:\n",
    "        print(\"\\n‚ö†Ô∏è Cannot check rhyme scheme due to incorrect word counts.\")\n",
    "        print(\"-\" * 30)\n",
    "        return\n",
    "\n",
    "    # --- Rule 3: Check Rhyme Scheme (4-3-2) ---\n",
    "    try:\n",
    "        word1_4 = words_by_line[0][3] # 4th word of 1st line\n",
    "        word2_3 = words_by_line[1][2] # 3rd word of 2nd line\n",
    "        word3_2 = words_by_line[2][1] # 2nd word of 3rd line\n",
    "        \n",
    "      \n",
    "\n",
    "        rhyme1 = get_rhyme_group(word1_4)\n",
    "        rhyme2 = get_rhyme_group(word2_3)\n",
    "        rhyme3 = get_rhyme_group(word3_2)\n",
    "        \n",
    "        print(f\"  - Rhyme part for '{word1_4}': '{rhyme1}'\")\n",
    "        print(f\"  - Rhyme part for '{word2_3}': '{rhyme2}'\")\n",
    "        print(f\"  - Rhyme part for '{word3_2}': '{rhyme3}'\")\n",
    "\n",
    "        if rhyme1 == rhyme2 == rhyme3:\n",
    "            print(\"\\n‚úÖ Rule Passed: The 4-3-2 rhyme scheme is correct.\")\n",
    "        else:\n",
    "            print(\"\\n‚ùå Rule Failed: The words do not rhyme correctly.\")\n",
    "\n",
    "    except IndexError:\n",
    "        # This should not happen if word count checks passed, but it's good practice\n",
    "        print(\"\\n‚ùå Error: Could not extract rhyming words. Check line structure.\")\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# --- Examples to Test ---\n",
    "\n",
    "# Example 1: A perfect Than-Bauk (by Ledƒ´ SayƒÅdaw)\n",
    "# Rhyme words: ·ÄÄ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏, ·Äú·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏, ·ÄÖ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏\n",
    "poem_correct = \"\"\"\n",
    "·ÄÅ·Ä≠·ÄØ·Ä∏·Äû·Ä¨·Ä∏·Äë·Ä¨·Ä∏·Äï·Äº\n",
    "·Äü·Ä∞·Äê·ÄØ·Ä∂·ÄÄ\n",
    "·Äô·ÄØ·ÄÅ·Äª·ÄÄ·Äº·Ä±·Ä¨·ÄÄ·Ä∫·Ä°·Äï·Ä∫·ÄÖ·ÄΩ·Ä¨\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_than_bauk(poem_correct)\n",
    "   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35a79e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Now you can try your own!\n",
      "Paste your 3-line poem below and press Enter twice to finish.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nNow you can try your own!\")\n",
    "print(\"Paste your 3-line poem below and press Enter twice to finish.\")\n",
    "\n",
    "user_input_lines = []\n",
    "while True:\n",
    "    try:\n",
    "        line = input()\n",
    "        if not line:\n",
    "            break\n",
    "        user_input_lines.append(line)\n",
    "    except EOFError:\n",
    "        break\n",
    "        \n",
    "user_poem = \"\\n\".join(user_input_lines)\n",
    "if user_poem:\n",
    "    check_than_bauk(user_poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330e5d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4801742b",
   "metadata": {},
   "source": [
    "\"·ÅÅ·Åã ·ÄÅ·Ä≠·ÄØ·Ä∏·Äû·Ä¨·Ä∏·Äë·Ä¨·Ä∏·Äï·Äº·Åä ·Äü·Ä∞·Äê·ÄØ·Ä∂·ÄÄ·Åä ·Äô·ÄØ·ÄÅ·Äª·ÄÄ·Äº·Ä±·Ä¨·ÄÄ·Ä∫·Ä°·Äï·Ä∫·ÄÖ·ÄΩ·Ä¨·Åã\n",
    "\n",
    "·ÅÇ·Åã ·ÄÄ·Ä≠·ÄØ·Äö·Ä∫·Äê·ÄΩ·ÄÑ·Ä∫·Ä∏·Äû·Ä∞·ÄÅ·Ä≠·ÄØ·Ä∏·Åä ·Äô·Äº·ÄÑ·Ä∫·Äû·Ä∞·ÄÅ·Ä≠·ÄØ·Ä∏·Åä ·Äî·Äæ·ÄÖ·Ä∫·Äô·Äª·Ä≠·ÄØ·Ä∏·Äô·Äæ·Äê·Ä∫·ÄÄ·Äº·Äõ·Ä¨·Åã\n",
    "\n",
    "·ÅÉ·Åã ·ÄÄ·Ä≠·ÄØ·Äö·Ä∫·Äê·ÄΩ·ÄÑ·Ä∫·Ä∏·Äë·Ä¨·Ä∏·Äï·Äº·Åä ·Äï·Äº·ÄÑ·Ä∫·Äë·Ä¨·Ä∏·Äï·Äº·Åä ·Äî·Äæ·ÄÖ·Ä∫·Äù·Äõ·Äæ·Ä≠·Äû·Ää·Ä∫·Äû·Ä¨·Åã\n",
    "\n",
    "·ÅÑ·Åã ·ÄÄ·Ä≠·ÄØ·Äö·Ä∫·Äê·ÄΩ·ÄÑ·Ä∫·Ä∏·Äõ·Äî·Ä∫·Äû·Ä∞·Åä ·Äï·Äº·ÄÑ·Ä∫·Äõ·Äî·Ä∫·Äû·Ä∞·Åä ·Äî·Äæ·ÄÖ·Ä∫·Äô·Ä∞·ÄÅ·ÄΩ·Ä≤·Äõ·Äæ·ÄØ·Äõ·Ä¨·Åã\n",
    "\n",
    "·ÅÖ·Åã ·Äï·Äº·ÄÑ·Ä∫·Äï·Äú·Ä∞·Äë·ÄÄ·Ä∫·Åä ·ÄÄ·Ä≠·ÄØ·Äö·Ä∫·Äê·ÄΩ·ÄÑ·Ä∫·Ä∏·ÄÅ·ÄÄ·Ä∫·Åä ·ÄÜ·ÄÄ·Ä∫·ÄÜ·ÄÄ·Ä∫·Äû·Ä≠·Ä°·Äï·Ä∫·ÄÖ·ÄΩ·Ä¨·Åã\n",
    "\n",
    "·ÅÜ·Åã ·ÄÄ·Ä≠·ÄØ·Äö·Ä∫·Äê·ÄΩ·ÄÑ·Ä∫·Ä∏·ÄÅ·Ä≠·ÄØ·Ä∏·Äû·Ä¨·Ä∏·Åä ·Äú·ÄÄ·Ä∫·ÄÅ·Ä∂·Äë·Ä¨·Ä∏·Åä ·Äï·Äº·ÄÑ·Ä∫·ÄÅ·Ä≠·ÄØ·Ä∏·Äû·Ä¨·Ä∏·Äê·ÄΩ·Ä± ·Äô·ÄΩ·Äæ·Ä±·Äú·Ä≠·Äô·Ä∑·Ä∫·Äô·Ää·Ä∫·Åã\n",
    "\n",
    "·Åá·Åã ·ÄÄ·Ä≠·ÄØ·Äö·Ä∫·Äê·ÄΩ·ÄÑ·Ä∫·Ä∏·Äë·Ä¨·Ä∏·Äï·Äº·Åä ·Äú·ÄÄ·Ä∫·ÄÅ·Ä∂·ÄÄ·Äº·Åä ·Äï·Äº·ÄÑ·Ä∫·ÄÄ·Äë·Ä¨·Ä∏·Äï·Äº-·ÄÅ·Äª·Ä±·Äú·Ä≠·Äô·Ä∑·Ä∫·Äô·Ää·Ä∫·Åã\n",
    "\n",
    "·Åà·Åã ·ÄÄ·Ä≠·ÄØ·Äö·Ä∫·Äê·ÄΩ·ÄÑ·Ä∫·Ä∏·Äõ·Äî·Ä∫·Äû·Ä∞·Åä ·Äú·ÄÄ·Ä∫·ÄÅ·Ä∂·Äô·Ä∞·Åä ·Äï·Äº·ÄÑ·Ä∫·Äõ·Äî·Ä∫·Äû·Ä∞·ÄÄ·Äº·Ä±·Ä¨·ÄÑ·Ä∑·Ä∫- ·Äû·Ä±·Äú·Ä≠·Äô·Ä∑·Ä∫·Äô·Ää·Ä∫·Åã \"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
