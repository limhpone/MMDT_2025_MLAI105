{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Burmese Poem Classification - Deep Learning Only\n",
        "\n",
        "A deep learning pipeline for classifying Burmese poems into traditional types using LSTM, CNN, and Transformer models.\n",
        "\n",
        "## Features:\n",
        "- Custom Burmese text preprocessing with syllable splitting\n",
        "- Deep learning models (LSTM, CNN, Transformer)\n",
        "- Character-level tokenization for Burmese text\n",
        "- Production-ready classifier with confidence scoring\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ All libraries imported successfully!\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import glob\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "import time\n",
        "from collections import Counter\n",
        "\n",
        "# Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (Dense, LSTM, Embedding, Dropout, Conv1D, \n",
        "                                    GlobalMaxPooling1D, Bidirectional, MultiHeadAttention, \n",
        "                                    LayerNormalization, Add, GlobalAveragePooling1D)\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Data processing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "print(\"✅ All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "def custom_syllable_splitter(text: str) -> list:\n",
        "    \"\"\"\n",
        "    Performs a deep syllable split by breaking down all consonant stacks.\n",
        "    This function uses a two-step process to achieve the required logic for\n",
        "    cases like 'နက္ခတ္တ' and 'ဥက္ကဋ္ဌ'.\n",
        "    \"\"\"\n",
        "\n",
        "    \n",
        "    # --- Step 1: Pre-processing to split stacks using a loop ---\n",
        "    # This loop is the only reliable way to handle chained stacks.\n",
        "    stacked_consonant_pattern = r'([က-အ])(်?္)([က-အ])'\n",
        "    processed_text = text\n",
        "    while re.search(stacked_consonant_pattern, processed_text):\n",
        "        processed_text = re.sub(stacked_consonant_pattern, r'\\1်'  + r'\\3', processed_text)\n",
        "    processed_text = re.sub(r\"(([A-Za-z0-9]+)|[က-အ|ဥ|ဦ](င်္|[က-အ|ဥ][ှ]*[့း]*[်]|္[က-အ]|[ါ-ှႏꩻ][ꩻ]*){0,}|.)\",r\"\\1 \", processed_text)\n",
        "    print()\n",
        "   #Step 2: Tokenization of the processed parts ---\n",
        "   # The string is now clean of stacks, so we can tokenize it reliably.\n",
        "    final_list = processed_text.split(\" \")\n",
        "    \n",
        "    # Filter out empty strings caused by trailing spaces\n",
        "    final_list = [word for word in final_list if word.strip()]\n",
        "        \n",
        "    return final_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Data loading functions defined!\n"
          ]
        }
      ],
      "source": [
        "# Data Loading Functions\n",
        "\n",
        "def load_excel_data(file_path='4. PoemSource\\Poem Excel files.xlsx'):\n",
        "    \"\"\"Load and process data from Excel file\"\"\"\n",
        "    print(\"Loading Excel data...\")\n",
        "    \n",
        "    try:\n",
        "        df = pd.read_excel(file_path)\n",
        "        print(f\"Excel data shape: {df.shape}\")\n",
        "        print(f\"Poem types (columns): {list(df.columns)}\")\n",
        "        \n",
        "        # Reshape data from wide to long format\n",
        "        excel_poems = []\n",
        "        \n",
        "        for col in df.columns:\n",
        "            for idx, row in df.iterrows():\n",
        "                poem_text = row[col]\n",
        "                if pd.notna(poem_text) and str(poem_text).strip():\n",
        "                    excel_poems.append({\n",
        "                        'title': f'Excel_Poem_{idx}_{col}',\n",
        "                        'author': 'Unknown',\n",
        "                        'poem_lines': [str(poem_text)],\n",
        "                        'poem_type': col,\n",
        "                        'source': 'excel',\n",
        "                        'notes': f'Row {idx}'\n",
        "                    })\n",
        "        \n",
        "        print(f\"Extracted {len(excel_poems)} poems from Excel\")\n",
        "        return excel_poems\n",
        "    \n",
        "    except FileNotFoundError:\n",
        "        print(f\"Excel file {file_path} not found. Skipping Excel data.\")\n",
        "        return []\n",
        "\n",
        "def load_json_data(json_dir='2 RawPoemJsonFromGemini'):\n",
        "    \"\"\"Load and process data from JSON files\"\"\"\n",
        "    print(\"Loading JSON data...\")\n",
        "    \n",
        "    json_poems = []\n",
        "    \n",
        "    if os.path.exists(json_dir):\n",
        "        json_files = glob.glob(os.path.join(json_dir, '*.json'))\n",
        "        print(f\"Found {len(json_files)} JSON files\")\n",
        "        \n",
        "        for json_file in json_files:\n",
        "            try:\n",
        "                with open(json_file, 'r', encoding='utf-8') as f:\n",
        "                    data = json.load(f)\n",
        "                \n",
        "                # Handle both single poem and list of poems\n",
        "                if isinstance(data, dict):\n",
        "                    data = [data]\n",
        "                \n",
        "                for poem in data:\n",
        "                    if 'poem_lines' in poem and 'poem_type' in poem:\n",
        "                        poem['source'] = 'json'\n",
        "                        json_poems.append(poem)\n",
        "                        \n",
        "            except Exception as e:\n",
        "                print(f\"Error loading {json_file}: {e}\")\n",
        "    else:\n",
        "        print(f\"JSON directory {json_dir} not found or empty\")\n",
        "    \n",
        "    print(f\"Extracted {len(json_poems)} poems from JSON files\")\n",
        "    return json_poems\n",
        "\n",
        "\n",
        "print(\"✅ Data loading functions defined!\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "လင်္ကာမှာ (၁) လင်္ကာ၊ (၂) ပျို့၊ (၃) မော်ကွန်း၊ (၄) သံပိုင်း၊ (၅) သမိုင်း၊ (၆) ရဲတင်း၊ (၇) တမ်းချင်း၊ (၈) ဧချင်းသံတိုင်၊ (၉) ဧချင်းသံပေါက်၊ (၁၀) ရတု၊ (၁၁) လူးတား၊ (၁၂) အန်၊ (၁၃) သံပေါက်၊ (၁၄) ဝဲရိုက်ဝဲသွင်းဟူ၍ ၁၄ မျိုး ရှိ၏။ \n",
        "\n",
        "သီချင်းတွင် (၁) ရကန်၊ (၂) ဟောစာ၊ (၃) သာချင်း၊ (၄) ကာချင်း၊(၅) အဲချင်း၊ (၆) အိုင်ချင်း၊ (၇) လေးချိုး၊ (၈) သံချို၊ (၉) ယိုးဒယား၊ (၁၀) ပတ်ပျိုး၊ (၁၁) ကြိုး၊ (၁၂) ဘွဲ့၊ (၁၃) ဘောလယ်အရိုး၊(၁၄) တေးအရိုးအဆန်း၊ (၁၅) သဖြန်အရိုးအဆန်း၊ (၁၆) ငိုချင်းအရိုးအဆန်း၊ (၁၇) တုံးချင်း အရိုးအဆန်း၊ (၁၈) ဟန်ချင်း အရိုး အဆန်း၊ (၁၉) လှေချင်း အရိုးအဆန်း၊(၂၀) နတ်သံ၊ (၂၁) မှာတမ်း၊ (၂၂) ဇာတ်စကားစပ်ပုံဟူ၍ ၂၂ မျိုးပါဝင်သည်။"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "def harmonize_poem_types():\n",
        "    \"\"\"\n",
        "    Creates a mapping to consolidate various Burmese poem types\n",
        "    into a smaller, more consistent set of categories.\n",
        "    \"\"\"\n",
        "    poem_type_mapping = {\n",
        "        # --- Modern Poetry (ခေတ်ပေါ်ကဗျာ) ---\n",
        "        # This category includes modern, free-form, and translated poetry.\n",
        "        'ခေတ်ပေါ်ကဗျာ': 'ခေတ်ပေါ်ကဗျာ',\n",
        "        'ရင်ဖွင့်ကဗျာ': 'ခေတ်ပေါ်ကဗျာ',\n",
        "        'ခေတ်စမ်းကဗျာ': 'ခေတ်ပေါ်ကဗျာ',\n",
        "        'Modern Burmese Poetry': 'ခေတ်ပေါ်ကဗျာ',\n",
        "        'ကဗျာတို': 'ခေတ်ပေါ်ကဗျာ',\n",
        "        'ကဗျာ': 'ခေတ်ပေါ်ကဗျာ',\n",
        "        'Prose Poem': 'ခေတ်ပေါ်ကဗျာ',\n",
        "        'Translation': 'ခေတ်ပေါ်ကဗျာ',\n",
        "        'ခေတ်စမ်းလင်္ကာ': 'ခေတ်ပေါ်ကဗျာ',\n",
        "        'တေးထပ်ညှပ် ဒွေးချိုးဆက်': 'ခေတ်ပေါ်ကဗျာ',\n",
        "        'လွတ်လပ်ကာရန်': 'ခေတ်ပေါ်ကဗျာ',\n",
        "        'လေးချိုးသဖြန်': 'ခေတ်ပေါ်ကဗျာ',\n",
        "        'ဘာသာပြန်ကဗျာ': 'ခေတ်ပေါ်ကဗျာ',\n",
        "        'Prose': 'ခေတ်ပေါ်ကဗျာ',\n",
        "        'Long-form Poem': 'ခေတ်ပေါ်ကဗျာ',\n",
        "        \n",
        "        # --- Classical Poetic Forms (လင်္ကာ) ---\n",
        "        # These are traditional poetic types with specific metrical rules.\n",
        "        'တေးကဗျာ': 'လင်္ကာ',\n",
        "        'လေးချိုး': 'လင်္ကာ',\n",
        "        'ဒွေးချိုး': 'လင်္ကာ',\n",
        "        'တောလား': 'လင်္ကာ',\n",
        "        'တြိချိုး': 'လင်္ကာ',\n",
        "        'ကလေးကဗျာ': 'လင်္ကာ',\n",
        "        'ရတု': 'လင်္ကာ',\n",
        "        'သံပေါက်': 'လင်္ကာ',\n",
        "        'ကဗျာရှည်': 'လင်္ကာ',\n",
        "        'ရတနာကြုတ်သွား': 'လင်္ကာ',\n",
        "        'ပျို့': 'လင်္ကာ',\n",
        "        'လူးတား': 'လင်္ကာ',\n",
        "        'အကျင့်ဝတ်ကဗျာ': 'လင်္ကာ',\n",
        "        'စာထောင့်လေးချိုး': 'လင်္ကာ',\n",
        "        'လေးချိုးကြီး': 'လင်္ကာ',\n",
        "        'ရတနာကြုတ်သွား (ကိုးလုံးဖွဲ့)': 'လင်္ကာ',\n",
        "        'လေးလုံးစပ်': 'လင်္ကာ',\n",
        "        'လေးဆစ်ချိုး': 'လင်္ကာ',\n",
        "        'လေးဆစ်': 'လင်္ကာ',\n",
        "        '၃-ဆစ်ချိုး': 'လင်္ကာ',\n",
        "        '၃ ကြော့': 'လင်္ကာ',\n",
        "        'ပိုဒ်စုံရတု': 'လင်္ကာ',\n",
        "        'Haiku': 'လင်္ကာ',\n",
        "        'နိဂုံးလင်္ကာ': 'လင်္ကာ',\n",
        "        \n",
        "        # --- Songs & Melodic Forms (သီချင်း) ---\n",
        "        # These are poem types intended to be sung or chanted.\n",
        "        'ကြိုးသီချင်း': 'သီချင်း',\n",
        "        'လွမ်းချင်း': 'သီချင်း',\n",
        "        'ဒုံးချင်း': 'သီချင်း',\n",
        "        'တမ်းချင်း': 'သီချင်း',\n",
        "        'တျာချင်း': 'သီချင်း',\n",
        "        'အိုင်ချင်း': 'သီချင်း',\n",
        "        'ညည်းချင်း': 'သီချင်း',\n",
        "        'ဘောလယ်': 'သီချင်း',\n",
        "        'သဖြန်': 'သီချင်း',\n",
        "        'အဲချင်း': 'သီချင်း',\n",
        "        'ကာချင်း': 'သီချင်း',\n",
        "        'ခွန်းထောက်': 'သီချင်း',\n",
        "        'ဟန်ချင်း': 'သီချင်း',\n",
        "        'မျိုးစောင့်တေး': 'သီချင်း',\n",
        "        'ပတ်ပျိုး': 'သီချင်း',\n",
        "        'ရကန်': 'သီချင်း',\n",
        "        'သံချို': 'သီချင်း',\n",
        "        'အန်ချင်း': 'သီချင်း',\n",
        "        'သံချိုလေးချိုး': 'သီချင်း',\n",
        "        'တိုက်ပွဲမှတ်တမ်းသီချင်း': 'သီချင်း',\n",
        "        'တပ်ထွက်သီချင်း': 'သီချင်း',\n",
        "        'သူရဲကောင်းသီချင်း': 'သီချင်း',\n",
        "        'အောင်ပွဲသီချင်း': 'သီချင်း',\n",
        "        'ဇာတိမာန်သီချင်း': 'သီချင်း',\n",
        "        'ငိုချင်း': 'သီချင်း',\n",
        "        'အလွမ်းသီချင်း': 'သီချင်း',\n",
        "        \n",
        "    \n",
        "        'တေးထပ်': 'သီချင်း',\n",
        "        'ပိဋကသွား': None,\n",
        "        'အမှာ': None,\n",
        "        \n",
        "        # --- Unrecognized/Error Values ---\n",
        "        # This seems to be an invalid entry from the source data.\n",
        "        None: None,\n",
        "    }\n",
        "    \n",
        "    return poem_type_mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ FIXED poem example functions ready!\n",
            "💡 Usage:\n",
            "  show_poem_examples_fixed()  # Show one example of each type (SAFE)\n",
            "  show_poem_examples_fixed(use_harmonized=False)  # Show original types\n",
            "  show_specific_type_fixed('ခေတ်စမ်းလင်္ကာ', num_examples=5)  # Show 5 examples\n"
          ]
        }
      ],
      "source": [
        "# FIXED: Show Examples of Each Poem Type (handles None values)\n",
        "\n",
        "def show_poem_examples_fixed(use_harmonized=True, max_lines=5):\n",
        "    \"\"\"\n",
        "    FIXED VERSION: Show actual examples of each poem type from the dataset\n",
        "    Now handles None values properly\n",
        "    \n",
        "    Args:\n",
        "        use_harmonized (bool): Use harmonized poem types or original types\n",
        "        max_lines (int): Maximum number of lines to show per poem\n",
        "    \"\"\"\n",
        "    print(\"📚 POEM TYPE EXAMPLES (FIXED VERSION)\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Load all data\n",
        "    all_poems = load_excel_data() + load_json_data()\n",
        "    \n",
        "    if not all_poems:\n",
        "        print(\"❌ No poems found!\")\n",
        "        return\n",
        "    \n",
        "    # Get type mapping if using harmonized types\n",
        "    type_mapping = harmonize_poem_types() if use_harmonized else {}\n",
        "    \n",
        "    # Group poems by type (handle None values)\n",
        "    poems_by_type = {}\n",
        "    for poem in all_poems:\n",
        "        original_type = poem.get('poem_type', 'Unknown')\n",
        "        \n",
        "        # Handle None values\n",
        "        if original_type is None:\n",
        "            original_type = 'Unknown'\n",
        "        \n",
        "        poem_type = type_mapping.get(original_type, original_type) if use_harmonized else original_type\n",
        "        \n",
        "        # Ensure poem_type is not None\n",
        "        if poem_type is None:\n",
        "            poem_type = 'Unknown'\n",
        "        \n",
        "        if poem_type not in poems_by_type:\n",
        "            poems_by_type[poem_type] = []\n",
        "        poems_by_type[poem_type].append(poem)\n",
        "    \n",
        "    # Show examples for each type\n",
        "    type_label = \"HARMONIZED\" if use_harmonized else \"ORIGINAL\"\n",
        "    print(f\"Showing examples from {len(poems_by_type)} {type_label} poem types:\\n\")\n",
        "    \n",
        "    for poem_type, poems in sorted(poems_by_type.items(), key=lambda x: len(x[1]), reverse=True):\n",
        "        # Safe handling of poem_type (should not be None now, but extra safety)\n",
        "        display_type = str(poem_type).upper() if poem_type else \"UNKNOWN\"\n",
        "        print(f\"🎭 {display_type} ({len(poems)} poems)\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        # Show first poem as example\n",
        "        example_poem = poems[0]\n",
        "        title = example_poem.get('title', 'No title') or 'No title'\n",
        "        author = example_poem.get('author', 'Unknown') or 'Unknown'\n",
        "        lines = example_poem.get('poem_lines', []) or []\n",
        "        \n",
        "        print(f\"📖 Title: {title}\")\n",
        "        print(f\"✍️  Author: {author}\")\n",
        "        print(f\"📝 Content:\")\n",
        "        \n",
        "        if isinstance(lines, list) and lines:\n",
        "            for i, line in enumerate(lines[:max_lines]):\n",
        "                if line and str(line).strip():\n",
        "                    print(f\"   {str(line).strip()}\")\n",
        "            if len(lines) > max_lines:\n",
        "                print(f\"   ... ({len(lines) - max_lines} more lines)\")\n",
        "        else:\n",
        "            content = str(lines)[:300] if lines else \"No content\"\n",
        "            print(f\"   {content}\")\n",
        "            if len(str(lines)) > 300:\n",
        "                print(\"   ...\")\n",
        "        \n",
        "        original_type_display = example_poem.get('poem_type', 'Unknown') or 'Unknown'\n",
        "        print(f\"🔢 Original type: {original_type_display}\")\n",
        "        print()\n",
        "\n",
        "def show_specific_type_fixed(poem_type, use_harmonized=True, num_examples=3):\n",
        "    \"\"\"\n",
        "    FIXED VERSION: Show multiple examples of a specific poem type\n",
        "    \"\"\"\n",
        "    if not poem_type:\n",
        "        print(\"❌ Please provide a poem type\")\n",
        "        return\n",
        "        \n",
        "    print(f\"📚 EXAMPLES OF: {str(poem_type).upper()}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    all_poems = load_excel_data() + load_json_data()\n",
        "    type_mapping = harmonize_poem_types() if use_harmonized else {}\n",
        "    \n",
        "    # Find poems of specified type\n",
        "    matching_poems = []\n",
        "    for poem in all_poems:\n",
        "        original_type = poem.get('poem_type', 'Unknown') or 'Unknown'\n",
        "        current_type = type_mapping.get(original_type, original_type) if use_harmonized else original_type\n",
        "        \n",
        "        if current_type and str(current_type).lower() == str(poem_type).lower():\n",
        "            matching_poems.append(poem)\n",
        "    \n",
        "    if not matching_poems:\n",
        "        print(f\"❌ No poems found for type: {poem_type}\")\n",
        "        return\n",
        "    \n",
        "    print(f\"Found {len(matching_poems)} poems of type '{poem_type}'\")\n",
        "    print(f\"Showing first {min(num_examples, len(matching_poems))} examples:\\n\")\n",
        "    \n",
        "    for i, poem in enumerate(matching_poems[:num_examples]):\n",
        "        print(f\"📖 Example {i+1}:\")\n",
        "        print(f\"   Title: {poem.get('title', 'No title') or 'No title'}\")\n",
        "        print(f\"   Author: {poem.get('author', 'Unknown') or 'Unknown'}\")\n",
        "        \n",
        "        lines = poem.get('poem_lines', []) or []\n",
        "        if isinstance(lines, list) and lines:\n",
        "            for line in lines[:3]:\n",
        "                if line and str(line).strip():\n",
        "                    print(f\"   {str(line).strip()}\")\n",
        "            if len(lines) > 3:\n",
        "                print(f\"   ... ({len(lines) - 3} more lines)\")\n",
        "        else:\n",
        "            content = str(lines)[:200] if lines else \"No content\"\n",
        "            print(f\"   {content}\")\n",
        "            if len(str(lines)) > 200:\n",
        "                print(\"   ...\")\n",
        "        print()\n",
        "\n",
        "print(\"✅ FIXED poem example functions ready!\")\n",
        "print(\"💡 Usage:\")\n",
        "print(\"  show_poem_examples_fixed()  # Show one example of each type (SAFE)\")\n",
        "print(\"  show_poem_examples_fixed(use_harmonized=False)  # Show original types\")\n",
        "print(\"  show_specific_type_fixed('ခေတ်စမ်းလင်္ကာ', num_examples=5)  # Show 5 examples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📚 POEM TYPE EXAMPLES (FIXED VERSION)\n",
            "============================================================\n",
            "Loading Excel data...\n",
            "Excel file 4. PoemSource\\Poem Excel files.xlsx not found. Skipping Excel data.\n",
            "Loading JSON data...\n",
            "JSON directory 2 RawPoemJsonFromGemini not found or empty\n",
            "Extracted 0 poems from JSON files\n",
            "❌ No poems found!\n"
          ]
        }
      ],
      "source": [
        "show_poem_examples_fixed(use_harmonized=True )  # Show original types\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ခိုးသားထားပြ၊ဟူတုံက၊မုချကြောက်အပ်စွာ။ ကိုယ်တွင်းသူခိုး၊မြင်သူခိုး၊နှစ်မျိုးမှတ်ကြရာ။ ကိုယ်တွင်းထားပြ၊ပြင်ထားပြ၊နှစ်ဝရှိသည်သာ။ ကိုယ်တွင်းရန်သူ၊ပြင်ရန်သူ၊နှစ်မူခွဲရှုရာ။ ပြင်ပလူထက်၊ကိုယ်တွင်းခက်၊ဆက်ဆက်သိအပ်စွာ။ ကိုယ်တွင်းခိုးသား၊လက်ခံထား၊ပြင်ခိုးသားတွေမွှေလိမ့်မည်။ ကိုယ်တွင်းထားပြ၊လက်ခံကြ၊ပြင်ကထားပြချေလိမ့်မည်။ ကိုယ်တွင်းရန်သူ၊လက်ခံမူ၊ပြင်ရန်သူကြောင့်သေလိမ့်မည်။\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def preprocess_poem_text(poem_lines, use_syllable_splitter=False):\n",
        "    \"\"\"Comprehensive text preprocessing for Burmese poems\"\"\"\n",
        "    if isinstance(poem_lines, list):\n",
        "        text = ' '.join(poem_lines)\n",
        "    else:\n",
        "        text = str(poem_lines)\n",
        "    \n",
        "\n",
        "    # 0. Basic cleaning: Multiple spaces to single space and strip\n",
        "    text = re.sub(r'\\s', '', text)\n",
        "    # print(text)\n",
        "    # 1. Remove newlines that follow '၊' or '။'\n",
        "    text = re.sub(r'([၊။])\\s*\\n*+', r'\\1', text)\n",
        "    # print(text)\n",
        "    \n",
        "    # 2. Add '။' if the next line starts with a Burmese number\n",
        "    text = re.sub(r'([၀-၉])', r'။ \\1', text)\n",
        "    # print(text)\n",
        "    # remove Englsih letters and numbers and special characters\n",
        "    text = re.sub(r'[A-Za-z0-9]', '', text)\n",
        "    # 3. Replace any remaining newlines with a single space\n",
        "    text = re.sub(r'\\n+', '၊', text)\n",
        "     # Replace two or more consecutive '၊' with a single '၊'\n",
        "    text = re.sub(r'၊{2,}', '၊', text)\n",
        "    \n",
        "    # Replace two or more consecutive '။' with a single '။'\n",
        "    text = re.sub(r'။{2,}', '။', text)\n",
        "\n",
        "    text =re.sub(r'([၀-၉]+)\\s*။', r'\\1', text)\n",
        "\n",
        "    text = re.sub(r'[၀-၉]+', '', text)\n",
        "    text = re.sub(r'[!@#$%^&*(),.?\":{}|<>~`\\'\\-_=+;\\\\/\\[\\]]', '', text)\n",
        "\n",
        "        # print(text)\n",
        "    text = re.sub(r'(\\s|^)။', '', text)\n",
        "\n",
        "    text = text.strip()\n",
        "    \n",
        "    # Note: The custom_syllable_splitter part was not included as its implementation\n",
        "    # was not provided, but you can add it back if needed.\n",
        "    # if use_syllable_splitter:\n",
        "    #     try:\n",
        "    #         syllables = custom_syllable_splitter(text)\n",
        "    #         text = ' '.join(syllables)\n",
        "    #     except:\n",
        "    #         pass # Fallback to original text if splitter fails\n",
        "    \n",
        "    return text\n",
        "\n",
        "poem_lines = [\n",
        "    \"\"\" ၁။ ခိုးသားထားပြ၊ ဟူတုံက၊ မုချကြောက်အပ်စွာ\n",
        "\n",
        "    \n",
        "    ၂။ ကိုယ်တွင်းသူခိုး၊ မြင်သူခိုး၊ နှစ်မျိုးမှတ်ကြရာ။\n",
        "\n",
        "    ၃။ ကိုယ်တွင်းထားပြ၊ ပြင်ထားပြ၊ နှစ်ဝရှိသည်သာ။\n",
        "\n",
        "    ၄။ ကိုယ်တွင်းရန်သူ၊ ပြင်ရန်သူ၊ နှစ်မူခွဲရှုရာ။\n",
        "\n",
        "    ၅။ ပြင်ပလူထက်၊ ကိုယ်တွင်းခက်၊ ဆက်ဆက်သိအပ်စွာ။\n",
        "\n",
        "    ၆။ ကိုယ်တွင်းခိုးသား၊ လက်ခံထား၊ ပြင်ခိုးသားတွေ မွှေလိမ့်မည်။\n",
        "\n",
        "    ၇။ ကိုယ်တွင်းထားပြ၊ လက်ခံကြ၊ ပြင်ကထားပြ-ချေလိမ့်မည်။\n",
        "\n",
        "    ၈။ ကိုယ်တွင်းရန်သူ၊ လက်ခံမူ၊ ပြင်ရန်သူကြောင့်- သေလိမ့်မည်။\n",
        "    \"\"\"\n",
        "]\n",
        "\n",
        "processed_text = preprocess_poem_text(poem_lines=poem_lines)\n",
        "print(processed_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Text processing functions defined!\n",
            "\n",
            "\n",
            "📊 Extracted features: {'num_lines': 24, 'num_stanzas': 8, 'total_syllables': 126}\n",
            "\n",
            "🔧 Feature summary:\n",
            "- Lines: 24\n",
            "- Stanzas: 8\n",
            "- Total syllables: 126\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "# from rhyme_chain_extractor import get_top_3_rhyme_schemes\n",
        "\n",
        "def extract_linguistic_features(processed_text):\n",
        "    \"\"\"Extract only essential features: lines number, stanza number, top 3 rhyme schemes, and syllable count\"\"\"\n",
        "    features = {}\n",
        "    \n",
        "    # Ensure we have a string input (compatible with preprocess_poem_text output)\n",
        "    if not isinstance(processed_text, str):\n",
        "        processed_text = str(processed_text) if processed_text is not None else \"\"\n",
        "    \n",
        "    # Count lines: any occurrence of '၊' or '။' indicates a line\n",
        "    line_count = processed_text.count('၊') + processed_text.count('။')\n",
        "    features['num_lines'] = max(1, line_count) if processed_text.strip() else 0\n",
        "    \n",
        "    # Count stanzas: only count '။' (Burmese full stop)\n",
        "    features['num_stanzas'] = processed_text.count('။')\n",
        "    \n",
        "    # Calculate total syllables using custom_syllable_splitter\n",
        "    try:\n",
        "        # Use custom_syllable_splitter for syllable counting\n",
        "        syllables = custom_syllable_splitter(processed_text)\n",
        "        features['total_syllables'] = len(syllables)\n",
        "    except NameError:\n",
        "        # Fallback if custom_syllable_splitter is not defined\n",
        "        features['total_syllables'] = 0\n",
        "    \n",
        "    # #Extract top 3 rhyme schemes using the rhyme chain extractor \n",
        "    # I was trying to include this for Machine learning model .\n",
        "    # try:\n",
        "    #     rhyme_result = get_top_3_rhyme_schemes(processed_text)\n",
        "        \n",
        "    #     # Extract top 3 schemes (just the scheme patterns, not counts)\n",
        "    #     top_3_schemes = [scheme for scheme, count in rhyme_result['top_3_schemes']]\n",
        "        \n",
        "    #     # Pad with \"None\" if less than 3 schemes found\n",
        "    #     while len(top_3_schemes) < 3:\n",
        "    #         top_3_schemes.append(\"None\")\n",
        "        \n",
        "    #     features['rhyme_scheme_1'] = top_3_schemes[0]  # Most frequent\n",
        "    #     features['rhyme_scheme_2'] = top_3_schemes[1]  # Second most frequent\n",
        "    #     features['rhyme_scheme_3'] = top_3_schemes[2]  # Third most frequent\n",
        "        \n",
        "    # except (ImportError, Exception):\n",
        "    #     # Fallback if rhyme extractor fails\n",
        "    #     features['rhyme_scheme_1'] = \"None\"\n",
        "    #     features['rhyme_scheme_2'] = \"None\" \n",
        "    #     features['rhyme_scheme_3'] = \"None\"\n",
        "    \n",
        "    return features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"✅ Text processing functions defined!\")\n",
        "\n",
        "# Test the updated feature extraction\n",
        "extracted_features = extract_linguistic_features(processed_text)\n",
        "print(f\"\\n📊 Extracted features: {extracted_features}\")\n",
        "\n",
        "# Show what features we now have\n",
        "print(f\"\\n🔧 Feature summary:\")\n",
        "print(f\"- Lines: {extracted_features.get('num_lines', 'N/A')}\")\n",
        "print(f\"- Stanzas: {extracted_features.get('num_stanzas', 'N/A')}\")\n",
        "print(f\"- Total syllables: {extracted_features.get('total_syllables', 'N/A')}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Data preprocessing functions defined!\n"
          ]
        }
      ],
      "source": [
        "# Data Preprocessing Functions\n",
        "\n",
        "def load_and_preprocess_data(excel_path='Poem Excel files.xlsx', json_dir='PoemJsonFiles'):\n",
        "    \"\"\"Load and combine all data sources\"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"LOADING POEM DATA\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Load Excel and JSON data\n",
        "    excel_poems = load_excel_data(excel_path)\n",
        "    json_poems = load_json_data(json_dir)\n",
        "    \n",
        "    print(f\"\\nTotal poems loaded:\")\n",
        "    print(f\"- Excel: {len(excel_poems)}\")\n",
        "    print(f\"- JSON: {len(json_poems)}\")\n",
        "    print(f\"- Combined: {len(excel_poems) + len(json_poems)}\")\n",
        "    \n",
        "    return excel_poems, json_poems\n",
        "\n",
        "def preprocess_all_poems(excel_poems, json_poems):\n",
        "    \"\"\"Preprocess and harmonize all data\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PREPROCESSING DATA\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    all_processed_poems = []\n",
        "    \n",
        "    # Process Excel poems\n",
        "    for poem in excel_poems:\n",
        "        processed_poem = {\n",
        "            'title': poem['title'],\n",
        "            'author': poem['author'],\n",
        "            'poem_type': poem['poem_type'],\n",
        "            'original_lines': poem['poem_lines'],\n",
        "            'source': poem['source']\n",
        "        }\n",
        "        \n",
        "        # Add preprocessed text\n",
        "        processed_poem['processed_text'] = preprocess_poem_text(poem['poem_lines'])\n",
        "        \n",
        "        # Add linguistic features\n",
        "        processed_poem.update(extract_linguistic_features(poem['poem_lines']))\n",
        "        \n",
        "        all_processed_poems.append(processed_poem)\n",
        "    \n",
        "    # Process JSON poems\n",
        "    for poem in json_poems:\n",
        "        processed_poem = {\n",
        "            'title': poem.get('title', 'Unknown'),\n",
        "            'author': poem.get('author', 'Unknown'),\n",
        "            'poem_type': poem.get('poem_type', 'Unknown'),\n",
        "            'original_lines': poem.get('poem_lines', []),\n",
        "            'source': poem.get('source', 'json')\n",
        "        }\n",
        "        \n",
        "        # Add preprocessed text\n",
        "        processed_poem['processed_text'] = preprocess_poem_text(poem.get('poem_lines', []))\n",
        "        \n",
        "        # Add linguistic features\n",
        "        processed_poem.update(extract_linguistic_features(poem.get('poem_lines', [])))\n",
        "        \n",
        "        all_processed_poems.append(processed_poem)\n",
        "    \n",
        "    # Create DataFrame\n",
        "    df_final = pd.DataFrame(all_processed_poems)\n",
        "    \n",
        "    # Apply poem type harmonization\n",
        "    poem_type_mapping = harmonize_poem_types()\n",
        "    df_final['poem_type'] = df_final['poem_type'].replace(poem_type_mapping)\n",
        "    \n",
        "    # Clean data - ensure poem types are strings and not None\n",
        "    df_clean = df_final[\n",
        "        (df_final['poem_type'] != 'Unknown') & \n",
        "        (df_final['poem_type'].notna()) &  # Remove Python None values\n",
        "        (df_final['poem_type'] != 'None') &  # Remove string 'None' values\n",
        "        (df_final['poem_type'] != None) &   # Extra safety for None values\n",
        "        (df_final['processed_text'].str.len() > 0)\n",
        "    ].copy()\n",
        "    \n",
        "    # Convert all poem types to strings to ensure uniform data type for label encoder\n",
        "    df_clean['poem_type'] = df_clean['poem_type'].astype(str)\n",
        "    \n",
        "    # Filter classes with minimum samples\n",
        "    min_samples_per_class = 5\n",
        "    class_counts = df_clean['poem_type'].value_counts()\n",
        "    valid_classes = class_counts[class_counts >= min_samples_per_class].index\n",
        "    df_model = df_clean[df_clean['poem_type'].isin(valid_classes)].copy()\n",
        "    \n",
        "    print(f\"Total processed poems: {len(df_final)}\")\n",
        "    print(f\"After cleaning: {len(df_clean)}\")\n",
        "    print(f\"Final dataset: {len(df_model)}\")\n",
        "    print(f\"Poem types: {df_model['poem_type'].nunique()}\")\n",
        "    \n",
        "    return df_model\n",
        "\n",
        "print(\"✅ Data preprocessing functions defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df_model = preprocess_all_poems(*load_and_preprocess_data())\n",
        "# print(f\"\\nFinal dataset shape: {df_model.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Data analysis functions defined!\n"
          ]
        }
      ],
      "source": [
        "# Data Analysis and Visualization Functions\n",
        "\n",
        "def analyze_dataset(df_model):\n",
        "    \"\"\"Analyze and visualize the dataset\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"DATA ANALYSIS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Basic statistics\n",
        "    print(f\"Dataset shape: {df_model.shape}\")\n",
        "    print(f\"Poem types: {df_model['poem_type'].nunique()}\")\n",
        "    print(f\"Authors: {df_model['author'].nunique()}\")\n",
        "    print(f\"Sources: {dict(df_model['source'].value_counts())}\")\n",
        "    \n",
        "    # Class distribution\n",
        "    print(\"\\nPoem type distribution:\")\n",
        "    type_dist = df_model['poem_type'].value_counts()\n",
        "    for ptype, count in type_dist.items():\n",
        "        print(f\"  {ptype}: {count}\")\n",
        "    \n",
        "    # Create visualization\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    fig.suptitle('Burmese Poem Dataset Analysis', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. Poem type distribution\n",
        "    poem_type_counts = df_model['poem_type'].value_counts().head(15)\n",
        "    axes[0, 0].barh(range(len(poem_type_counts)), poem_type_counts.values)\n",
        "    axes[0, 0].set_yticks(range(len(poem_type_counts)))\n",
        "    axes[0, 0].set_yticklabels(poem_type_counts.index, fontsize=8)\n",
        "    axes[0, 0].set_xlabel('Number of Poems')\n",
        "    axes[0, 0].set_title('Top 15 Poem Types Distribution')\n",
        "    axes[0, 0].grid(axis='x', alpha=0.3)\n",
        "    \n",
        "    # 2. Source distribution\n",
        "    source_counts = df_model['source'].value_counts()\n",
        "    colors = ['#ff9999', '#66b3ff']\n",
        "    axes[0, 1].pie(source_counts.values, labels=source_counts.index, autopct='%1.1f%%', \n",
        "                   colors=colors, startangle=90)\n",
        "    axes[0, 1].set_title('Data Source Distribution')\n",
        "    \n",
        "    # 3. Poem length distribution\n",
        "    axes[1, 0].hist(df_model['num_lines'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    axes[1, 0].set_xlabel('Number of Lines')\n",
        "    axes[1, 0].set_ylabel('Frequency')\n",
        "    axes[1, 0].set_title('Poem Length Distribution (Lines)')\n",
        "    axes[1, 0].grid(alpha=0.3)\n",
        "    \n",
        "    # 4. Syllable distribution\n",
        "    axes[1, 1].hist(df_model['total_syllables'], bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "    axes[1, 1].set_xlabel('Total Syllables')\n",
        "    axes[1, 1].set_ylabel('Frequency')\n",
        "    axes[1, 1].set_title('Syllable Count Distribution')\n",
        "    axes[1, 1].grid(alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return df_model\n",
        "\n",
        "print(\"✅ Data analysis functions defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "# analyze_dataset(df_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Feature preparation functions defined!\n"
          ]
        }
      ],
      "source": [
        "# Feature Preparation Functions\n",
        "\n",
        "def prepare_features_for_deep_learning(df_model):\n",
        "    \"\"\"Prepare features for deep learning models\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FEATURE PREPARATION FOR DEEP LEARNING\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Prepare text and labels\n",
        "    X_text = df_model['processed_text'].values\n",
        "    y = df_model['poem_type'].values\n",
        "    \n",
        "    # Encode labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_encoded = label_encoder.fit_transform(y)\n",
        "    \n",
        "    # Split data\n",
        "    X_text_train, X_text_test, y_train, y_test = train_test_split(\n",
        "        X_text, y_encoded,\n",
        "        test_size=0.2, \n",
        "        random_state=42,\n",
        "        stratify=y_encoded\n",
        "    )\n",
        "    \n",
        "    print(f\"Training set size: {len(X_text_train)}\")\n",
        "    print(f\"Test set size: {len(X_text_test)}\")\n",
        "    print(f\"Classes: {len(label_encoder.classes_)}\")\n",
        "    print(f\"Class names: {label_encoder.classes_}\")\n",
        "    \n",
        "    return X_text_train, X_text_test, y_train, y_test, label_encoder\n",
        "\n",
        "def create_sequences_for_deep_learning(X_text_train, X_text_test, y_train, y_test, label_encoder):\n",
        "    \"\"\"Create sequences for deep learning models\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"CREATING SEQUENCES FOR DEEP LEARNING\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Set random seeds for reproducibility\n",
        "    tf.random.set_seed(42)\n",
        "    np.random.seed(42)\n",
        "    \n",
        " \n",
        "    X_train_syllables = [' '.join(custom_syllable_splitter(poem)) for poem in X_text_train]\n",
        "    X_test_syllables = [' '.join(custom_syllable_splitter(poem)) for poem in X_text_test]\n",
        "    tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "    tokenizer.fit_on_texts(X_train_syllables)\n",
        "\n",
        "    # Convert text to sequences\n",
        "    X_train_seq = tokenizer.texts_to_sequences(X_train_syllables)\n",
        "    X_test_seq = tokenizer.texts_to_sequences(X_test_syllables)\n",
        "    \n",
        "    # Pad sequences to same length\n",
        "    max_length = min(500, max(max(len(seq) for seq in X_train_seq), max(len(seq) for seq in X_test_seq)))\n",
        "    X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
        "    X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
        "    \n",
        "    # Convert labels to categorical\n",
        "    y_train_cat = to_categorical(y_train, num_classes=len(label_encoder.classes_))\n",
        "    y_test_cat = to_categorical(y_test, num_classes=len(label_encoder.classes_))\n",
        "    \n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    num_classes = len(label_encoder.classes_)\n",
        "    \n",
        "    print(f\"Vocabulary size: {vocab_size}\")\n",
        "    print(f\"Max sequence length: {max_length}\")\n",
        "    print(f\"Number of classes: {num_classes}\")\n",
        "    print(f\"Training sequences shape: {X_train_padded.shape}\")\n",
        "    print(f\"Test sequences shape: {X_test_padded.shape}\")\n",
        "    \n",
        "    return (X_train_padded, X_test_padded, y_train_cat, y_test_cat, \n",
        "            tokenizer, max_length, vocab_size, num_classes)\n",
        "\n",
        "print(\"✅ Feature preparation functions defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Deep learning model creation functions defined!\n"
          ]
        }
      ],
      "source": [
        "# Deep Learning Model Creation Functions\n",
        "\n",
        "def create_lstm_model(vocab_size, max_length, num_classes, embedding_dim=128, lstm_units=64):\n",
        "    \"\"\"Create LSTM model for poem classification\"\"\"\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        Bidirectional(LSTM(lstm_units, dropout=0.3, recurrent_dropout=0.3)),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "def create_cnn_model(vocab_size, max_length, num_classes, embedding_dim=128):\n",
        "    \"\"\"Create CNN model for poem classification\"\"\"\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
        "        Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
        "        GlobalMaxPooling1D(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "def create_transformer_model(vocab_size, max_length, num_classes, embedding_dim=128, num_heads=4):\n",
        "    \"\"\"Create Transformer model for poem classification\"\"\"\n",
        "    # Input layer\n",
        "    inputs = tf.keras.Input(shape=(max_length,))\n",
        "    \n",
        "    # Embedding layer\n",
        "    embedding_layer = Embedding(vocab_size, embedding_dim)(inputs)\n",
        "    \n",
        "    # Positional encoding (simplified)\n",
        "    positions = tf.range(start=0, limit=max_length, delta=1)\n",
        "    positions = Embedding(max_length, embedding_dim)(positions)\n",
        "    x = embedding_layer + positions\n",
        "    \n",
        "    # Transformer block\n",
        "    attention_output = MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embedding_dim//num_heads\n",
        "    )(x, x)\n",
        "    \n",
        "    # Add & Norm\n",
        "    x = Add()([x, attention_output])\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "    \n",
        "    # Feed Forward Network\n",
        "    ffn_output = Dense(embedding_dim*2, activation='relu')(x)\n",
        "    ffn_output = Dense(embedding_dim)(ffn_output)\n",
        "    \n",
        "    # Add & Norm\n",
        "    x = Add()([x, ffn_output])\n",
        "    x = LayerNormalization(epsilon=1e-6)(x)\n",
        "    \n",
        "    # Global average pooling\n",
        "    x = GlobalAveragePooling1D()(x)\n",
        "    \n",
        "    # Classification head\n",
        "    x = Dense(128, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "    \n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "print(\"✅ Deep learning model creation functions defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Model training functions defined!\n"
          ]
        }
      ],
      "source": [
        "# Model Training Functions\n",
        "\n",
        "def train_and_evaluate_model(model, model_name, X_train, y_train, X_test, y_test, epochs=20, batch_size=32):\n",
        "    \"\"\"Train and evaluate a deep learning model\"\"\"\n",
        "    print(f\"\\n=== Training {model_name} ===\")\n",
        "    \n",
        "    # Callbacks\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
        "    \n",
        "    # Build model first to get parameters count\n",
        "    try:\n",
        "        param_count = model.count_params()\n",
        "    except ValueError:\n",
        "        model.build(input_shape=(None, X_train.shape[1]))\n",
        "        param_count = model.count_params()\n",
        "    \n",
        "    print(f\"{model_name} parameters: {param_count:,}\")\n",
        "    \n",
        "    # Train model\n",
        "    start_time = time.time()\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_test, y_test),\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[early_stopping, reduce_lr],\n",
        "        verbose=1\n",
        "    )\n",
        "    training_time = time.time() - start_time\n",
        "    \n",
        "    # Evaluate model\n",
        "    start_time = time.time()\n",
        "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "    inference_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"\\n=== {model_name} Results ===\")\n",
        "    print(f\"Accuracy: {test_acc:.4f}\")\n",
        "    print(f\"Loss: {test_loss:.4f}\")\n",
        "    print(f\"Training time: {training_time:.2f} seconds\")\n",
        "    print(f\"Inference time: {inference_time:.4f} seconds\")\n",
        "    \n",
        "    return {\n",
        "        'model': model,\n",
        "        'history': history,\n",
        "        'accuracy': test_acc,\n",
        "        'loss': test_loss,\n",
        "        'training_time': training_time,\n",
        "        'inference_time': inference_time,\n",
        "        'param_count': param_count\n",
        "    }\n",
        "\n",
        "def train_all_deep_learning_models(X_train_padded, X_test_padded, y_train_cat, y_test_cat, \n",
        "                                  vocab_size, max_length, num_classes):\n",
        "    \"\"\"Train all deep learning models and compare results\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TRAINING DEEP LEARNING MODELS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    models_results = {}\n",
        "    \n",
        "    # 1. Train LSTM model\n",
        "    print(\"\\n1. Creating and training LSTM model...\")\n",
        "    lstm_model = create_lstm_model(vocab_size, max_length, num_classes)\n",
        "    models_results['LSTM'] = train_and_evaluate_model(\n",
        "        lstm_model, 'LSTM', X_train_padded, y_train_cat, X_test_padded, y_test_cat, epochs=20, batch_size=32\n",
        "    )\n",
        "    \n",
        "    # 2. Train CNN model\n",
        "    print(\"\\n2. Creating and training CNN model...\")\n",
        "    cnn_model = create_cnn_model(vocab_size, max_length, num_classes)\n",
        "    models_results['CNN'] = train_and_evaluate_model(\n",
        "        cnn_model, 'CNN', X_train_padded, y_train_cat, X_test_padded, y_test_cat, epochs=20, batch_size=32\n",
        "    )\n",
        "    \n",
        "    # 3. Train Transformer model\n",
        "    print(\"\\n3. Creating and training Transformer model...\")\n",
        "    transformer_model = create_transformer_model(vocab_size, max_length, num_classes)\n",
        "    models_results['Transformer'] = train_and_evaluate_model(\n",
        "        transformer_model, 'Transformer', X_train_padded, y_train_cat, X_test_padded, y_test_cat, \n",
        "        epochs=15, batch_size=16  # Fewer epochs and smaller batch for Transformer\n",
        "    )\n",
        "    \n",
        "    # Find best model\n",
        "    best_model_name = max(models_results.keys(), key=lambda x: models_results[x]['accuracy'])\n",
        "    best_accuracy = models_results[best_model_name]['accuracy']\n",
        "    best_model = models_results[best_model_name]['model']\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MODEL COMPARISON RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    results_data = []\n",
        "    for model_name, result in models_results.items():\n",
        "        results_data.append({\n",
        "            'Model': model_name,\n",
        "            'Accuracy': result['accuracy'],\n",
        "            'Loss': result['loss'],\n",
        "            'Parameters': result['param_count'],\n",
        "            'Training Time (s)': result['training_time'],\n",
        "            'Inference Time (s)': result['inference_time']\n",
        "        })\n",
        "    \n",
        "    results_df = pd.DataFrame(results_data).sort_values('Accuracy', ascending=False)\n",
        "    print(results_df.to_string(index=False, float_format='%.4f'))\n",
        "    \n",
        "    print(f\"\\n🏆 Best model: {best_model_name}\")\n",
        "    print(f\"🏆 Best accuracy: {best_accuracy:.4f}\")\n",
        "    \n",
        "    return models_results, best_model_name, best_model, best_accuracy\n",
        "\n",
        "print(\"✅ Model training functions defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Visualization functions defined!"
          ]
        }
      ],
      "source": [
        "# Visualization Functions\n",
        "\n",
        "def plot_training_histories(models_results):\n",
        "    \"\"\"Plot training histories for deep learning models\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('Deep Learning Model Training Histories', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    colors = ['blue', 'red', 'green']\n",
        "    \n",
        "    # Plot training & validation accuracy\n",
        "    axes[0, 0].set_title('Model Accuracy')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Accuracy')\n",
        "    \n",
        "    for i, (model_name, result) in enumerate(models_results.items()):\n",
        "        history = result['history']\n",
        "        epochs = range(1, len(history.history['accuracy']) + 1)\n",
        "        axes[0, 0].plot(epochs, history.history['accuracy'], \n",
        "                       color=colors[i], linestyle='-', label=f'{model_name} Train')\n",
        "        axes[0, 0].plot(epochs, history.history['val_accuracy'], \n",
        "                       color=colors[i], linestyle='--', label=f'{model_name} Val')\n",
        "    \n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot training & validation loss\n",
        "    axes[0, 1].set_title('Model Loss')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    \n",
        "    for i, (model_name, result) in enumerate(models_results.items()):\n",
        "        history = result['history']\n",
        "        epochs = range(1, len(history.history['loss']) + 1)\n",
        "        axes[0, 1].plot(epochs, history.history['loss'], \n",
        "                       color=colors[i], linestyle='-', label=f'{model_name} Train')\n",
        "        axes[0, 1].plot(epochs, history.history['val_loss'], \n",
        "                       color=colors[i], linestyle='--', label=f'{model_name} Val')\n",
        "    \n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Model comparison bar chart - Accuracy\n",
        "    model_names = list(models_results.keys())\n",
        "    accuracies = [models_results[model]['accuracy'] for model in model_names]\n",
        "    \n",
        "    axes[1, 0].bar(model_names, accuracies, color=colors[:len(model_names)])\n",
        "    axes[1, 0].set_title('Model Accuracy Comparison')\n",
        "    axes[1, 0].set_ylabel('Accuracy')\n",
        "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Add accuracy values on bars\n",
        "    for i, acc in enumerate(accuracies):\n",
        "        axes[1, 0].text(i, acc + 0.01, f'{acc:.3f}', ha='center', va='bottom')\n",
        "    \n",
        "    # Training time comparison\n",
        "    training_times = [models_results[model]['training_time'] for model in model_names]\n",
        "    axes[1, 1].bar(model_names, training_times, color=colors[:len(model_names)])\n",
        "    axes[1, 1].set_title('Training Time Comparison')\n",
        "    axes[1, 1].set_ylabel('Time (seconds)')\n",
        "    axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # Add time values on bars\n",
        "    for i, time_val in enumerate(training_times):\n",
        "        axes[1, 1].text(i, time_val + max(training_times)*0.01, f'{time_val:.1f}s', \n",
        "                       ha='center', va='bottom')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_model_comparison_detailed(models_results):\n",
        "    \"\"\"Create detailed comparison plots for all models\"\"\"\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    fig.suptitle('Detailed Model Performance Analysis', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    model_names = list(models_results.keys())\n",
        "    colors = ['blue', 'red', 'green']\n",
        "    \n",
        "    # 1. Accuracy comparison\n",
        "    accuracies = [models_results[model]['accuracy'] for model in model_names]\n",
        "    axes[0, 0].bar(model_names, accuracies, color=colors[:len(model_names)])\n",
        "    axes[0, 0].set_title('Test Accuracy')\n",
        "    axes[0, 0].set_ylabel('Accuracy')\n",
        "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "    for i, acc in enumerate(accuracies):\n",
        "        axes[0, 0].text(i, acc + 0.005, f'{acc:.3f}', ha='center', va='bottom')\n",
        "    \n",
        "    # 2. Loss comparison\n",
        "    losses = [models_results[model]['loss'] for model in model_names]\n",
        "    axes[0, 1].bar(model_names, losses, color=colors[:len(model_names)])\n",
        "    axes[0, 1].set_title('Test Loss')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "    for i, loss in enumerate(losses):\n",
        "        axes[0, 1].text(i, loss + max(losses)*0.01, f'{loss:.3f}', ha='center', va='bottom')\n",
        "    \n",
        "    # 3. Parameters comparison\n",
        "    params = [models_results[model]['param_count'] for model in model_names]\n",
        "    axes[0, 2].bar(model_names, params, color=colors[:len(model_names)])\n",
        "    axes[0, 2].set_title('Model Parameters')\n",
        "    axes[0, 2].set_ylabel('Parameter Count')\n",
        "    axes[0, 2].grid(axis='y', alpha=0.3)\n",
        "    for i, param in enumerate(params):\n",
        "        axes[0, 2].text(i, param + max(params)*0.01, f'{param/1000:.0f}K', ha='center', va='bottom')\n",
        "    \n",
        "    # 4. Training time comparison\n",
        "    train_times = [models_results[model]['training_time'] for model in model_names]\n",
        "    axes[1, 0].bar(model_names, train_times, color=colors[:len(model_names)])\n",
        "    axes[1, 0].set_title('Training Time')\n",
        "    axes[1, 0].set_ylabel('Time (seconds)')\n",
        "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
        "    for i, time_val in enumerate(train_times):\n",
        "        axes[1, 0].text(i, time_val + max(train_times)*0.01, f'{time_val:.1f}s', ha='center', va='bottom')\n",
        "    \n",
        "    # 5. Inference time comparison\n",
        "    inf_times = [models_results[model]['inference_time'] for model in model_names]\n",
        "    axes[1, 1].bar(model_names, inf_times, color=colors[:len(model_names)])\n",
        "    axes[1, 1].set_title('Inference Time')\n",
        "    axes[1, 1].set_ylabel('Time (seconds)')\n",
        "    axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "    for i, time_val in enumerate(inf_times):\n",
        "        axes[1, 1].text(i, time_val + max(inf_times)*0.01, f'{time_val:.3f}s', ha='center', va='bottom')\n",
        "    \n",
        "    # 6. Efficiency score (accuracy / training_time)\n",
        "    efficiency = [models_results[model]['accuracy'] / (models_results[model]['training_time'] + 1) \n",
        "                 for model in model_names]\n",
        "    axes[1, 2].bar(model_names, efficiency, color=colors[:len(model_names)])\n",
        "    axes[1, 2].set_title('Efficiency Score (Accuracy/Time)')\n",
        "    axes[1, 2].set_ylabel('Efficiency')\n",
        "    axes[1, 2].grid(axis='y', alpha=0.3)\n",
        "    for i, eff in enumerate(efficiency):\n",
        "        axes[1, 2].text(i, eff + max(efficiency)*0.01, f'{eff:.4f}', ha='center', va='bottom')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"✅ Visualization functions defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Prediction functions defined!"
          ]
        }
      ],
      "source": [
        "# Prediction Functions\n",
        "\n",
        "def predict_poem_type(poem_lines, model, tokenizer, max_length, label_encoder, return_confidence=False):\n",
        "    \"\"\"Predict poem type for new poem\"\"\"\n",
        "    # Preprocess text\n",
        "    processed_text = preprocess_poem_text(poem_lines, use_syllable_splitter=True)\n",
        "    \n",
        "    # Convert to sequence\n",
        "    sequence = tokenizer.texts_to_sequences([processed_text])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
        "    \n",
        "    # Predict\n",
        "    prediction_proba = model.predict(padded_sequence, verbose=0)[0]\n",
        "    prediction = np.argmax(prediction_proba)\n",
        "    confidence = np.max(prediction_proba)\n",
        "    \n",
        "    poem_type = label_encoder.inverse_transform([prediction])[0]\n",
        "    \n",
        "    if return_confidence:\n",
        "        return poem_type, confidence\n",
        "    return poem_type\n",
        "\n",
        "def predict_top_k_poem_types(poem_lines, model, tokenizer, max_length, label_encoder, k=3):\n",
        "    \"\"\"Get top k predictions with confidence scores\"\"\"\n",
        "    # Preprocess text\n",
        "    processed_text = preprocess_poem_text(poem_lines, use_syllable_splitter=True)\n",
        "    \n",
        "    # Convert to sequence\n",
        "    sequence = tokenizer.texts_to_sequences([processed_text])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
        "    \n",
        "    # Predict\n",
        "    probabilities = model.predict(padded_sequence, verbose=0)[0]\n",
        "    \n",
        "    # Get top k indices\n",
        "    top_k_indices = np.argsort(probabilities)[-k:][::-1]\n",
        "    \n",
        "    results = []\n",
        "    for idx in top_k_indices:\n",
        "        poem_type = label_encoder.inverse_transform([idx])[0]\n",
        "        confidence = probabilities[idx]\n",
        "        results.append((poem_type, confidence))\n",
        "    \n",
        "    return results\n",
        "\n",
        "def create_production_classifier(best_model, tokenizer, max_length, label_encoder, best_model_name):\n",
        "    \"\"\"Create a production-ready classifier function\"\"\"\n",
        "    def classifier(poem_lines, return_confidence=False, top_k=None):\n",
        "        \"\"\"\n",
        "        Production classifier function\n",
        "        \n",
        "        Args:\n",
        "            poem_lines: List of poem lines or single string\n",
        "            return_confidence: If True, return confidence score\n",
        "            top_k: If provided, return top k predictions\n",
        "        \n",
        "        Returns:\n",
        "            Prediction result based on parameters\n",
        "        \"\"\"\n",
        "        if top_k:\n",
        "            return predict_top_k_poem_types(poem_lines, best_model, tokenizer, max_length, label_encoder, k=top_k)\n",
        "        else:\n",
        "            return predict_poem_type(poem_lines, best_model, tokenizer, max_length, label_encoder, return_confidence)\n",
        "    \n",
        "    # Add metadata to the classifier function\n",
        "    classifier.model_name = best_model_name\n",
        "    classifier.model = best_model\n",
        "    classifier.tokenizer = tokenizer\n",
        "    classifier.max_length = max_length\n",
        "    classifier.label_encoder = label_encoder\n",
        "    \n",
        "    return classifier\n",
        "\n",
        "print(\"✅ Prediction functions defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Complete pipeline function defined!\n"
          ]
        }
      ],
      "source": [
        "# Complete Pipeline Function\n",
        "\n",
        "def run_complete_deep_learning_pipeline(excel_path='Poem Excel files.xlsx', json_dir='2.RawPoemJsonFromGemini'):\n",
        "    \"\"\"Run the complete deep learning classification pipeline\"\"\"\n",
        "    print(\"🧠 STARTING BURMESE POEM CLASSIFICATION - DEEP LEARNING PIPELINE\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # 1. Load and preprocess data\n",
        "    excel_poems, json_poems = load_and_preprocess_data(excel_path, json_dir)\n",
        "    df_model = preprocess_all_poems(excel_poems, json_poems)\n",
        "    \n",
        "    # 2. Analyze dataset\n",
        "    analyze_dataset(df_model)\n",
        "    \n",
        "    # 3. Prepare features for deep learning\n",
        "    X_text_train, X_text_test, y_train, y_test, label_encoder = prepare_features_for_deep_learning(df_model)\n",
        "    \n",
        "    # 4. Create sequences for deep learning\n",
        "    (X_train_padded, X_test_padded, y_train_cat, y_test_cat, \n",
        "     tokenizer, max_length, vocab_size, num_classes) = create_sequences_for_deep_learning(\n",
        "        X_text_train, X_text_test, y_train, y_test, label_encoder\n",
        "    )\n",
        "    \n",
        "    # 5. Train all deep learning models\n",
        "    models_results, best_model_name, best_model, best_accuracy = train_all_deep_learning_models(\n",
        "        X_train_padded, X_test_padded, y_train_cat, y_test_cat, vocab_size, max_length, num_classes\n",
        "    )\n",
        "    \n",
        "    # 6. Plot training histories and comparisons\n",
        "    plot_training_histories(models_results)\n",
        "    plot_model_comparison_detailed(models_results)\n",
        "    \n",
        "    # 7. Create production classifier\n",
        "    classifier = create_production_classifier(\n",
        "        best_model, tokenizer, max_length, label_encoder, best_model_name\n",
        "    )\n",
        "    \n",
        "    print(\"\\n🎉 DEEP LEARNING PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"✅ Best model: {best_model_name}\")\n",
        "    print(f\"✅ Accuracy: {best_accuracy:.4f}\")\n",
        "    print(f\"✅ Classes supported: {len(label_encoder.classes_)}\")\n",
        "    print(f\"✅ Classifier ready for production!\")\n",
        "    \n",
        "    return {\n",
        "        'classifier': classifier,\n",
        "        'models_results': models_results,\n",
        "        'best_model': best_model,\n",
        "        'best_model_name': best_model_name,\n",
        "        'best_accuracy': best_accuracy,\n",
        "        'tokenizer': tokenizer,\n",
        "        'max_length': max_length,\n",
        "        'label_encoder': label_encoder,\n",
        "        'df_model': df_model\n",
        "    }\n",
        "\n",
        "print(\"✅ Complete pipeline function defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🚀 Run the Pipeline\n",
        "\n",
        "Execute the complete deep learning pipeline by running the cell below. This will:\n",
        "\n",
        "1. **Load Data** - Load poems from Excel and JSON files\n",
        "2. **Preprocess** - Clean and harmonize poem data\n",
        "3. **Analyze** - Generate visualizations and statistics\n",
        "4. **Prepare Features** - Create sequences for deep learning\n",
        "5. **Train Models** - Train LSTM, CNN, and Transformer models\n",
        "6. **Compare Results** - Visualize model performance\n",
        "7. **Create Classifier** - Build production-ready classifier\n",
        "\n",
        "**Note:** This process may take several minutes depending on your dataset size and hardware.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧠 STARTING BURMESE POEM CLASSIFICATION - DEEP LEARNING PIPELINE\n",
            "================================================================================\n",
            "================================================================================\n",
            "LOADING POEM DATA\n",
            "================================================================================\n",
            "Loading Excel data...\n"
          ]
        },
        {
          "ename": "PermissionError",
          "evalue": "[Errno 13] Permission denied: '2.RawPoemJsonFromGemini'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run the Complete Pipeline\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m pipeline_results = \u001b[43mrun_complete_deep_learning_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexcel_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m2.RawPoemJsonFromGemini\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m4. PoemSource\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mPoemExcelfiles.xlsx\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Extract results\u001b[39;00m\n\u001b[32m      8\u001b[39m classifier = pipeline_results[\u001b[33m'\u001b[39m\u001b[33mclassifier\u001b[39m\u001b[33m'\u001b[39m]\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mrun_complete_deep_learning_pipeline\u001b[39m\u001b[34m(excel_path, json_dir)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 1. Load and preprocess data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m excel_poems, json_poems = \u001b[43mload_and_preprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexcel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m df_model = preprocess_all_poems(excel_poems, json_poems)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# 2. Analyze dataset\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mload_and_preprocess_data\u001b[39m\u001b[34m(excel_path, json_dir)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Load Excel and JSON data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m excel_poems = \u001b[43mload_excel_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexcel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m json_poems = load_json_data(json_dir)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTotal poems loaded:\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mload_excel_data\u001b[39m\u001b[34m(file_path)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading Excel data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExcel data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPoem types (columns): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(df.columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\phyom\\.virtualenvs\\poems-AgVwVVEp\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[32m    494\u001b[39m     should_close = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     io = \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine != io.engine:\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\phyom\\.virtualenvs\\poems-AgVwVVEp\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[39m, in \u001b[36mExcelFile.__init__\u001b[39m\u001b[34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   1548\u001b[39m     ext = \u001b[33m\"\u001b[39m\u001b[33mxls\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     ext = \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m   1552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1554\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1555\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mExcel file format cannot be determined, you must specify \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1556\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33man engine manually.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1557\u001b[39m         )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\phyom\\.virtualenvs\\poems-AgVwVVEp\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[39m, in \u001b[36minspect_excel_format\u001b[39m\u001b[34m(content_or_path, storage_options)\u001b[39m\n\u001b[32m   1399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m   1400\u001b[39m     content_or_path = BytesIO(content_or_path)\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m   1404\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[32m   1405\u001b[39m     stream = handle.handle\n\u001b[32m   1406\u001b[39m     stream.seek(\u001b[32m0\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\phyom\\.virtualenvs\\poems-AgVwVVEp\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
            "\u001b[31mPermissionError\u001b[39m: [Errno 13] Permission denied: '2.RawPoemJsonFromGemini'"
          ]
        }
      ],
      "source": [
        "# Run the Complete Pipeline\n",
        "pipeline_results = run_complete_deep_learning_pipeline(\n",
        "    excel_path='2.RawPoemJsonFromGemini',\n",
        "    json_dir='4. PoemSource\\PoemExcelfiles.xlsx'\n",
        ")\n",
        "\n",
        "# Extract results\n",
        "classifier = pipeline_results['classifier']\n",
        "best_model_name = pipeline_results['best_model_name']\n",
        "best_accuracy = pipeline_results['best_accuracy']\n",
        "\n",
        "print(f\"\\n🎯 Pipeline completed! Best model: {best_model_name} with {best_accuracy:.4f} accuracy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 🧪 Test the Classifier\n",
        "\n",
        "Now let's test our trained classifier with some example Burmese poems!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Testing Sample Poem 1:\n",
            "  1. ပြည်တွင်းစစ်ကြောင့် ပြေးရတဲ့သူတွေ\n",
            "  2. အိမ်မှာ ပြန်မရောက်နိုင်ကြတော့ဘူး\n",
            "  3. တိုင်းပြည်ကို သနားမိတယ်\n",
            "  4. ငြိမ်းချမ်းရေး လွမ်းမိတယ်\n",
            "\n",
            "📊 Predicted poem type: ခေတ်ပေါ်ကဗျာ\n",
            "📊 Predicted type: ခေတ်ပေါ်ကဗျာ\n",
            "📊 Confidence: 0.9999\n",
            "\n",
            "📊 Top 3 most likely poem types:\n",
            "  1. ခေတ်ပေါ်ကဗျာ: 0.9999\n",
            "  2. လင်္ကာ: 0.0001\n",
            "  3. သီချင်း: 0.0000\n"
          ]
        }
      ],
      "source": [
        "# Test with Sample Poems\n",
        "\n",
        "# Test Poem 1: Modern poem about civil war\n",
        "sample_poem_1 = [\n",
        "    \"ပြည်တွင်းစစ်ကြောင့် ပြေးရတဲ့သူတွေ\",\n",
        "    \"အိမ်မှာ ပြန်မရောက်နိုင်ကြတော့ဘူး\",\n",
        "    \"တိုင်းပြည်ကို သနားမိတယ်\",\n",
        "    \"ငြိမ်းချမ်းရေး လွမ်းမိတယ်\"\n",
        "]\n",
        "\n",
        "print(\"🔍 Testing Sample Poem 1:\")\n",
        "for i, line in enumerate(sample_poem_1, 1):\n",
        "    print(f\"  {i}. {line}\")\n",
        "\n",
        "# Simple prediction\n",
        "predicted_type = classifier(sample_poem_1)\n",
        "print(f\"\\n📊 Predicted poem type: {predicted_type}\")\n",
        "\n",
        "# Prediction with confidence\n",
        "predicted_type, confidence = classifier(sample_poem_1, return_confidence=True)\n",
        "print(f\"📊 Predicted type: {predicted_type}\")\n",
        "print(f\"📊 Confidence: {confidence:.4f}\")\n",
        "\n",
        "# Top 3 predictions\n",
        "top_3 = classifier(sample_poem_1, top_k=3)\n",
        "print(f\"\\n📊 Top 3 most likely poem types:\")\n",
        "for i, (poem_type, conf) in enumerate(top_3, 1):\n",
        "    print(f\"  {i}. {poem_type}: {conf:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "🔍 Testing Sample Poem 2 (Traditional style):\n",
            "  1. နံနက်မိုးပေါက်သံ ကြားရတယ်\n",
            "  2. ငှက်သံများ သီဆိုနေတယ်\n",
            "  3. စိတ်ကို အေးစေတယ်\n",
            "  4. ငြိမ်းချမ်းတယ်\n",
            "\n",
            "📊 Predicted type: ခေတ်ပေါ်ကဗျာ\n",
            "📊 Confidence: 0.9999\n",
            "\n",
            "==================================================\n",
            "🔍 Testing Sample Poem 3 (Short emotional):\n",
            "  1. မိုးရွာနေတဲ့ညမှာ\n",
            "  2. စိတ်ညစ်တယ်\n",
            "  3. အိမ်ပြန်ချင်တယ်\n",
            "\n",
            "📊 Top 3 predictions:\n",
            "  1. ခေတ်ပေါ်ကဗျာ: 0.9999\n",
            "  2. လင်္ကာ: 0.0001\n",
            "  3. သီချင်း: 0.0000\n"
          ]
        }
      ],
      "source": [
        "# Test with Different Types of Poems\n",
        "\n",
        "# Test Poem 2: Traditional four-line poem\n",
        "sample_poem_2 = [\n",
        "    \"နံနက်မိုးပေါက်သံ ကြားရတယ်\",\n",
        "    \"ငှက်သံများ သီဆိုနေတယ်\",\n",
        "    \"စိတ်ကို အေးစေတယ်\",\n",
        "    \"ငြိမ်းချမ်းတယ်\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"🔍 Testing Sample Poem 2 (Traditional style):\")\n",
        "for i, line in enumerate(sample_poem_2, 1):\n",
        "    print(f\"  {i}. {line}\")\n",
        "\n",
        "predicted_type, confidence = classifier(sample_poem_2, return_confidence=True)\n",
        "print(f\"\\n📊 Predicted type: {predicted_type}\")\n",
        "print(f\"📊 Confidence: {confidence:.4f}\")\n",
        "\n",
        "# Test Poem 3: Short emotional poem\n",
        "sample_poem_3 = [\n",
        "    \"မိုးရွာနေတဲ့ညမှာ\",\n",
        "    \"စိတ်ညစ်တယ်\", \n",
        "    \"အိမ်ပြန်ချင်တယ်\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"🔍 Testing Sample Poem 3 (Short emotional):\")\n",
        "for i, line in enumerate(sample_poem_3, 1):\n",
        "    print(f\"  {i}. {line}\")\n",
        "\n",
        "top_3 = classifier(sample_poem_3, top_k=3)\n",
        "print(f\"\\n📊 Top 3 predictions:\")\n",
        "for i, (poem_type, conf) in enumerate(top_3, 1):\n",
        "    print(f\"  {i}. {poem_type}: {conf:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 📈 Model Performance Summary\n",
        "\n",
        "Let's examine the performance of all our deep learning models:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏆 DEEP LEARNING MODEL PERFORMANCE SUMMARY\n",
            "================================================================================\n",
            "      Model Accuracy   Loss Parameters Training Time Inference Time\n",
            "       LSTM   0.6783 0.6696    484,099        190.9s        0.7519s\n",
            "        CNN   0.8000 0.5276    403,811         10.5s        0.1320s\n",
            "Transformer   0.8217 0.5971    517,763        166.4s        0.8936s\n",
            "\n",
            "🥇 Best performing model: Transformer\n",
            "🥇 Best accuracy: 0.8217\n",
            "🥇 Supported classes: 3\n",
            "\n",
            "📋 Available poem types:\n",
            "   1. ခေတ်ပေါ်ကဗျာ\n",
            "   2. လင်္ကာ\n",
            "   3. သီချင်း\n"
          ]
        }
      ],
      "source": [
        "# Model Performance Summary\n",
        "models_results = pipeline_results['models_results']\n",
        "\n",
        "print(\"🏆 DEEP LEARNING MODEL PERFORMANCE SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "performance_data = []\n",
        "for model_name, result in models_results.items():\n",
        "    performance_data.append({\n",
        "        'Model': model_name,\n",
        "        'Accuracy': f\"{result['accuracy']:.4f}\",\n",
        "        'Loss': f\"{result['loss']:.4f}\",\n",
        "        'Parameters': f\"{result['param_count']:,}\",\n",
        "        'Training Time': f\"{result['training_time']:.1f}s\",\n",
        "        'Inference Time': f\"{result['inference_time']:.4f}s\"\n",
        "    })\n",
        "\n",
        "performance_df = pd.DataFrame(performance_data)\n",
        "print(performance_df.to_string(index=False))\n",
        "\n",
        "print(f\"\\n🥇 Best performing model: {pipeline_results['best_model_name']}\")\n",
        "print(f\"🥇 Best accuracy: {pipeline_results['best_accuracy']:.4f}\")\n",
        "print(f\"🥇 Supported classes: {len(pipeline_results['label_encoder'].classes_)}\")\n",
        "\n",
        "print(f\"\\n📋 Available poem types:\")\n",
        "for i, class_name in enumerate(pipeline_results['label_encoder'].classes_, 1):\n",
        "    print(f\"  {i:2d}. {class_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📁 Model can be saved as: best_burmese_poem_classifier_transformer.h5\n",
            "📁 Components can be saved as: poem_classifier_components.pkl\n",
            "📁 Best model: Transformer with 0.8217 accuracy\n",
            "\n",
            "📖 To load the classifier later:\n",
            "```python\n",
            "import tensorflow as tf\n",
            "import pickle\n",
            "\n",
            "# Load model\n",
            "model = tf.keras.models.load_model('best_burmese_poem_classifier_transformer.h5')\n",
            "\n",
            "# Load components\n",
            "with open('poem_classifier_components.pkl', 'rb') as f:\n",
            "    components = pickle.load(f)\n",
            "\n",
            "# Create classifier\n",
            "classifier = create_production_classifier(\n",
            "    model, components['tokenizer'], components['max_length'], \n",
            "    components['label_encoder'], components['model_name']\n",
            ")\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# Save the Best Model and Components \n",
        "\n",
        "import pickle\n",
        "import joblib\n",
        "\n",
        "# Save the best model\n",
        "best_model = pipeline_results['best_model']\n",
        "tokenizer = pipeline_results['tokenizer']\n",
        "label_encoder = pipeline_results['label_encoder']\n",
        "max_length = pipeline_results['max_length']\n",
        "\n",
        "# Save model (uncomment to save)\n",
        "# best_model.save('best_burmese_poem_classifier.h5')\n",
        "print(f\"📁 Model can be saved as: best_burmese_poem_classifier_{pipeline_results['best_model_name'].lower()}.h5\")\n",
        "\n",
        "# Save tokenizer and other components (uncomment to save)\n",
        "# with open('poem_classifier_components.pkl', 'wb') as f:\n",
        "#     pickle.dump({\n",
        "#         'tokenizer': tokenizer,\n",
        "#         'label_encoder': label_encoder,\n",
        "#         'max_length': max_length,\n",
        "#         'model_name': pipeline_results['best_model_name']\n",
        "#     }, f)\n",
        "\n",
        "print(f\"📁 Components can be saved as: poem_classifier_components.pkl\")\n",
        "print(f\"📁 Best model: {pipeline_results['best_model_name']} with {pipeline_results['best_accuracy']:.4f} accuracy\")\n",
        "\n",
        "# Usage instructions\n",
        "print(f\"\\n📖 To load the classifier later:\")\n",
        "print(f\"```python\")\n",
        "print(f\"import tensorflow as tf\")\n",
        "print(f\"import pickle\")\n",
        "print(f\"\")\n",
        "print(f\"# Load model\")\n",
        "print(f\"model = tf.keras.models.load_model('best_burmese_poem_classifier_{pipeline_results['best_model_name'].lower()}.h5')\")\n",
        "print(f\"\")\n",
        "print(f\"# Load components\")\n",
        "print(f\"with open('poem_classifier_components.pkl', 'rb') as f:\")\n",
        "print(f\"    components = pickle.load(f)\")\n",
        "print(f\"\")\n",
        "print(f\"# Create classifier\")\n",
        "print(f\"classifier = create_production_classifier(\")\n",
        "print(f\"    model, components['tokenizer'], components['max_length'], \")\n",
        "print(f\"    components['label_encoder'], components['model_name']\")\n",
        "print(f\")\")\n",
        "print(f\"```\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "poems-AgVwVVEp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
