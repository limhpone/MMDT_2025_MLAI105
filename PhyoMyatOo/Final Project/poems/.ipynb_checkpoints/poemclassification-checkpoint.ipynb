{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8249bafd",
   "metadata": {},
   "source": [
    "# Burmese Poem Classification with Deep Learning\n",
    "\n",
    "This notebook implements a comprehensive deep learning system for classifying Burmese poems into different types.\n",
    "\n",
    "## Data Sources:\n",
    "1. **Excel File**: `NLP Project.xlsx` - Contains 45 poems across 35 different poem types\n",
    "2. **JSON Files**: Extracted poems from PDF using OCR + Gemini 2.5\n",
    "\n",
    "## Approach:\n",
    "- Multi-model ensemble (LSTM, CNN, Transformer)\n",
    "- Transfer learning with multilingual models\n",
    "- Custom Burmese text preprocessing\n",
    "- Data augmentation for small dataset handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09854103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.18\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ededce20",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# ML libraries\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split, StratifiedKFold, cross_val_score\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, LSTM, GRU, Conv1D, MaxPooling1D, GlobalMaxPooling1D,\n",
    "    Embedding, Dropout, BatchNormalization, Bidirectional,\n",
    "    Input, Concatenate, Attention\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Text processing\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c254519b",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c63ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the custom syllable splitter from existing notebook\n",
    "def custom_syllable_splitter(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Performs a deep syllable split by breaking down all consonant stacks.\n",
    "    This function uses a two-step process to achieve the required logic for\n",
    "    cases like 'နက္ခတ္တ' and 'ဥက္ကဋ္ဌ'.\n",
    "    \"\"\"\n",
    "    # Step 1: Pre-processing to split stacks using a loop\n",
    "    stacked_consonant_pattern = r'([က-အ])(်?္)([က-အ])'\n",
    "    processed_text = text\n",
    "    while re.search(stacked_consonant_pattern, processed_text):\n",
    "        processed_text = re.sub(stacked_consonant_pattern, r'\\1်'  + r'\\3', processed_text)\n",
    "    processed_text = re.sub(r\"(([A-Za-z0-9]+)|[က-အ|ဥ|ဦ](င်္|[က-အ|ဥ][ှ]*[့း]*[်]|္[က-အ]|[ါ-ှႏꩻ][ꩻ]*){0,}|.)\",r\"\\1 \", processed_text)\n",
    "    \n",
    "    # Step 2: Tokenization of the processed parts\n",
    "    final_list = processed_text.split(\" \")\n",
    "    \n",
    "    # Filter out empty strings caused by trailing spaces\n",
    "    final_list = [word for word in final_list if word.strip()]\n",
    "        \n",
    "    return final_list\n",
    "\n",
    "print(\"Custom syllable splitter loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7836ee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BurmeseTextPreprocessor:\n",
    "    \"\"\"\n",
    "    Comprehensive text preprocessor for Burmese poems\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.burmese_consonants = \"ကခဂဃငစဆဇဈညဋဌဍဎဏတထဒဓနပဖဗဘမယရလဝသဟဠအ\"\n",
    "        self.consonantal_medials = \"ျြှွ\"\n",
    "        \n",
    "    def normalize_unicode(self, text):\n",
    "        \"\"\"Normalize Unicode text to handle different encodings\"\"\"\n",
    "        return unicodedata.normalize('NFC', text)\n",
    "    \n",
    "    def clean_ocr_errors(self, text):\n",
    "        \"\"\"Clean common OCR errors in Burmese text\"\"\"\n",
    "        # Remove extra whitespaces\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Remove unwanted characters but keep Burmese text, numbers, and basic punctuation\n",
    "        text = re.sub(r'[^\\u1000-\\u109F\\u0020-\\u007E၀-၉]', '', text)\n",
    "        \n",
    "        # Fix common number format issues\n",
    "        text = re.sub(r'(၁|၂|၃|၄|၅|၆|၇|၈|၉|၀)\\s*[။|၊]', r'\\1။', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def preprocess_poem_lines(self, poem_lines):\n",
    "        \"\"\"Preprocess a list of poem lines\"\"\"\n",
    "        if isinstance(poem_lines, str):\n",
    "            poem_lines = [poem_lines]\n",
    "        \n",
    "        processed_lines = []\n",
    "        for line in poem_lines:\n",
    "            if pd.isna(line) or line is None:\n",
    "                continue\n",
    "            \n",
    "            # Basic cleaning\n",
    "            line = str(line)\n",
    "            line = self.normalize_unicode(line)\n",
    "            line = self.clean_ocr_errors(line)\n",
    "            \n",
    "            if line:  # Only add non-empty lines\n",
    "                processed_lines.append(line)\n",
    "        \n",
    "        return processed_lines\n",
    "    \n",
    "    def extract_syllables(self, text):\n",
    "        \"\"\"Extract syllables using the custom syllable splitter\"\"\"\n",
    "        return custom_syllable_splitter(text)\n",
    "    \n",
    "    def get_poem_features(self, poem_lines):\n",
    "        \"\"\"Extract various features from poem lines\"\"\"\n",
    "        if not poem_lines:\n",
    "            return {\n",
    "                'total_lines': 0,\n",
    "                'total_syllables': 0,\n",
    "                'avg_line_length': 0,\n",
    "                'avg_syllables_per_line': 0,\n",
    "                'text_content': ''\n",
    "            }\n",
    "        \n",
    "        total_lines = len(poem_lines)\n",
    "        combined_text = ' '.join(poem_lines)\n",
    "        total_chars = len(combined_text)\n",
    "        \n",
    "        # Get syllables for the entire poem\n",
    "        all_syllables = self.extract_syllables(combined_text)\n",
    "        total_syllables = len(all_syllables)\n",
    "        \n",
    "        return {\n",
    "            'total_lines': total_lines,\n",
    "            'total_syllables': total_syllables,\n",
    "            'avg_line_length': total_chars / total_lines if total_lines > 0 else 0,\n",
    "            'avg_syllables_per_line': total_syllables / total_lines if total_lines > 0 else 0,\n",
    "            'text_content': combined_text\n",
    "        }\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = BurmeseTextPreprocessor()\n",
    "print(\"Burmese text preprocessor initialized!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ad9dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_excel_data(file_path='NLP Project.xlsx'):\n",
    "    \"\"\"Load and process data from Excel file\"\"\"\n",
    "    print(\"Loading Excel data...\")\n",
    "    \n",
    "    df = pd.read_excel(file_path)\n",
    "    print(f\"Excel data shape: {df.shape}\")\n",
    "    print(f\"Poem types (columns): {list(df.columns)}\")\n",
    "    \n",
    "    # Reshape data from wide to long format\n",
    "    excel_poems = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        for idx, row in df.iterrows():\n",
    "            poem_text = row[col]\n",
    "            if pd.notna(poem_text) and str(poem_text).strip():\n",
    "                excel_poems.append({\n",
    "                    'title': f'Excel_Poem_{idx}_{col}',\n",
    "                    'author': 'Unknown',\n",
    "                    'poem_lines': [str(poem_text)],\n",
    "                    'poem_type': col,\n",
    "                    'source': 'excel',\n",
    "                    'notes': f'Row {idx}'\n",
    "                })\n",
    "    \n",
    "    print(f\"Extracted {len(excel_poems)} poems from Excel\")\n",
    "    return excel_poems\n",
    "\n",
    "def load_json_data(json_dir='PoemJsonFiles'):\n",
    "    \"\"\"Load and process data from JSON files\"\"\"\n",
    "    print(\"Loading JSON data...\")\n",
    "    \n",
    "    json_poems = []\n",
    "    \n",
    "    # Check if directory exists and has files\n",
    "    if os.path.exists(json_dir):\n",
    "        json_files = glob.glob(os.path.join(json_dir, '*.json'))\n",
    "        print(f\"Found {len(json_files)} JSON files\")\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            try:\n",
    "                with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                # Handle both single poem and list of poems\n",
    "                if isinstance(data, dict):\n",
    "                    data = [data]\n",
    "                \n",
    "                for poem in data:\n",
    "                    if 'poem_lines' in poem and 'poem_type' in poem:\n",
    "                        poem['source'] = 'json'\n",
    "                        json_poems.append(poem)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {json_file}: {e}\")\n",
    "    else:\n",
    "        print(f\"JSON directory {json_dir} not found or empty\")\n",
    "    \n",
    "    print(f\"Extracted {len(json_poems)} poems from JSON files\")\n",
    "    return json_poems\n",
    "\n",
    "def create_sample_json_data():\n",
    "    \"\"\"Create sample JSON data based on your example\"\"\"\n",
    "    sample_poems = [\n",
    "        {\n",
    "            \"title\": \"ပန်းသဇင်\",\n",
    "            \"author\": \"မောင်အေးမောင်\",\n",
    "            \"language\": \"Burmese\",\n",
    "            \"poem_lines\": [\n",
    "                \"၁၊ ခါနွေလေပြန်၊ သူရကန်မူး\",\n",
    "                \"ဆူထန်တက်ကြွေ၊ ပူလျှံငွေကို၊\",\n",
    "                \"မတွေ့သဘော၊ နှဲမြော၏သို့၊\",\n",
    "                \"မဇ္ဈိသည့်သွင်၊ ညွန့်မရှင်သို။\"\n",
    "            ],\n",
    "            \"notes\": \"ဂန္ထလောက ၁၂၉၄-ခု။ တော်သလင်းလ\",\n",
    "            \"release_date\": \"1294, တော်သလင်းလ\",\n",
    "            \"poem_type\": \"ခေတ်စမ်းကဗျာ\"\n",
    "        },\n",
    "        {\n",
    "            \"title\": \"ကံနဲသူနတ်ရှင်နောင်\",\n",
    "            \"author\": \"မောင်အေးမောင်\",\n",
    "            \"language\": \"Burmese\", \n",
    "            \"poem_lines\": [\n",
    "                \"ပင်တိုင်စံခာတုကလျှာအနှံ့ငယ်ကြောင့်၊ ကြင်ပိုင်ရန်-စာအညကဗျာဖွဲ့ရှာရ၊\",\n",
    "                \"မဟာသမုဒြိယာခဲ့ပါတဲ့အကြောင်းခံ၊\",\n",
    "                \"အသက်လိုအချစ်ထူးရော့ထင့်၊ ပုရစ်ဖူးတွေဝေဝေမြိုင်တော့၊\"\n",
    "            ],\n",
    "            \"notes\": \"ဂန္ထလောက ၁၂၉၄-ခု၊တော်သလင်းလက\",\n",
    "            \"release_date\": \"1294, တော်သလင်းလ\",\n",
    "            \"poem_type\": \"ခေတ်စမ်းကဗျာ\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return sample_poems\n",
    "\n",
    "# Load all data\n",
    "excel_poems = load_excel_data()\n",
    "json_poems = load_json_data()\n",
    "\n",
    "# If no JSON data found, use sample data for demonstration\n",
    "if not json_poems:\n",
    "    print(\"No JSON files found, using sample data for demonstration...\")\n",
    "    json_poems = create_sample_json_data()\n",
    "\n",
    "print(f\"\\nTotal poems loaded:\")\n",
    "print(f\"- Excel: {len(excel_poems)}\")\n",
    "print(f\"- JSON: {len(json_poems)}\")\n",
    "print(f\"- Combined: {len(excel_poems) + len(json_poems)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_pipenv-meVFCTpa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
