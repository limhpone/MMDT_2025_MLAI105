{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\nuwai\\anaconda3\\envs\\myenv\\lib\\site-packages (from nltk) (1.5.1)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\nuwai\\appdata\\roaming\\python\\python39\\site-packages (from click->nltk) (0.4.4)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading regex-2024.11.6-cp39-cp39-win_amd64.whl (274 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.8 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install openpyxl for reading excel\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data_cleaning as dc\n",
    "# dc.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in our pipeline involves loading the data into a pandas DataFrame. This is accomplished using the pandas library, which is imported at the beginning of the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the data\n",
    "import pandas as pd\n",
    "import data_preprocessing as dp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/MMNames_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dp.clean_name_column(df, 'name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #filter out very short name\n",
    "# df = df[df['name'].str.len() >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\n",
      "Nyaung Pin Thar    34\n",
      "Nyaung Kone        32\n",
      "Thar Yar Kone      28\n",
      "Ywar Thit          28\n",
      "Kan Gyi            25\n",
      "                   ..\n",
      "Moe Sit             1\n",
      "Kyauk Ku Pyin       1\n",
      "Amarapura           1\n",
      "Win Kan             1\n",
      "Pyin Shey           1\n",
      "Name: count, Length: 3624, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Returns only duplicated names (appears more than once)\n",
    "duplicated_names = df['name'][df['name'].duplicated()].value_counts()\n",
    "print(duplicated_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "duplicated_names = df['name'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\n",
      "Thar Yar Kone    13\n",
      "Ywar Thit        12\n",
      "Nyaung Kone      11\n",
      "Kyar Inn         10\n",
      "Tha Yet Taw      10\n",
      "                 ..\n",
      "Nam Yang          2\n",
      "Nawng Hpar        2\n",
      "Hway Aw           2\n",
      "Bant Bar          2\n",
      "Inn Din           2\n",
      "Name: count, Length: 1331, dtype: int64\n",
      "           SR_Name             name\n",
      "1085    Ayeyarwady  Nyaung Pin Thar\n",
      "2770   Bago (East)  Nyaung Pin Thar\n",
      "3645   Bago (West)  Nyaung Pin Thar\n",
      "4447        Kachin  Nyaung Pin Thar\n",
      "5970        Magway  Nyaung Pin Thar\n",
      "7127      Mandalay  Nyaung Pin Thar\n",
      "8936   Nay Pyi Taw  Nyaung Pin Thar\n",
      "10233      Sagaing  Nyaung Pin Thar\n",
      "14177       Yangon  Nyaung Pin Thar\n"
     ]
    }
   ],
   "source": [
    "duplicated_names = duplicated_names[duplicated_names > 1]\n",
    "print(duplicated_names)\n",
    "print(df[df['name'] == 'Nyaung Pin Thar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14832, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                Bogale\n",
       "1               Danubyu\n",
       "2                Dedaye\n",
       "3                 Einme\n",
       "4                Du Yar\n",
       "              ...      \n",
       "19503    Ka Lawng Waing\n",
       "19504          Par Hkar\n",
       "19505           Man War\n",
       "19506         Gyeik Taw\n",
       "19511         Laung Zin\n",
       "Name: name, Length: 14832, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg tokens: 2.471331127169582\n",
      "Max tokens: 9\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seq_lengths = [len(s.split()) for s in df['name']]\n",
    "print(\"Avg tokens:\", np.mean(seq_lengths))\n",
    "print(\"Max tokens:\", np.max(seq_lengths))\n",
    "\n",
    "\n",
    "max_len = max(df['name'].apply(len))\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = df.copy() # crate another copy for differnt tokenizaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SR_Name\n",
       "Ayeyarwady      1627\n",
       "Bago (East)      757\n",
       "Bago (West)      659\n",
       "Chin             562\n",
       "Kachin           740\n",
       "Kayah            187\n",
       "Kayin            465\n",
       "Magway          1590\n",
       "Mandalay        1397\n",
       "Mon              468\n",
       "Nay Pyi Taw      204\n",
       "Rakhine         1060\n",
       "Sagaing         2205\n",
       "Shan (East)      226\n",
       "Shan (North)    1132\n",
       "Shan (South)     511\n",
       "Tanintharyi      390\n",
       "Yangon           627\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.groupby('SR_Name')['name'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SR_Name\n",
       "Sagaing         0.148916\n",
       "Ayeyarwady      0.109880\n",
       "Magway          0.107382\n",
       "Mandalay        0.094347\n",
       "Shan (North)    0.076450\n",
       "Rakhine         0.071588\n",
       "Bago (East)     0.051124\n",
       "Kachin          0.049976\n",
       "Bago (West)     0.044506\n",
       "Yangon          0.042345\n",
       "Chin            0.037955\n",
       "Shan (South)    0.034511\n",
       "Mon             0.031607\n",
       "Kayin           0.031404\n",
       "Tanintharyi     0.026339\n",
       "Shan (East)     0.015263\n",
       "Nay Pyi Taw     0.013777\n",
       "Kayah           0.012629\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df['SR_Name'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10364, 12473) (4443, 12473)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Prepare the data\n",
    "clean_df = dp.preprocess_category(clean_df,'SR_Name')\n",
    "clean_df = dp.preprocess_onehot(clean_df,'name')\n",
    "\n",
    "y = clean_df['SR_Name'].values \n",
    "X = clean_df.drop(columns=['SR_Name']).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y) \n",
    "print(X_train.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv(\"clean_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(df['SR_Name'])\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# new_df['SR_Name_encoded'] = le.fit_transform(new_df['SR_Name'])\n",
    "# region_for_class_12 = le.inverse_transform([12])[0]\n",
    "# print(\"Class 12 corresponds to region:\", region_for_class_12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a baseline model with one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Build a NN model with TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_onehot_basic_classification_model(input_shape, num_classes, params={}):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 1. Define your early stopping callback\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',     # You can also monitor 'val_accuracy'\n",
    "    patience=5,             # Number of epochs with no improvement after which training will stop\n",
    "    restore_best_weights=True # Restore weights from the epoch with the best value of the monitored metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss',      # Watch validation loss\n",
    "    factor=0.5,              # Reduce LR by this factor\n",
    "    patience=3,              # Wait 3 epochs before reducing\n",
    "    min_lr=1e-6,             # Donâ€™t go below this\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.5055116573992782, 1: 1.0863731656184485, 2: 1.2489756567847674, 3: 1.4650834040147018, 4: 1.1115401115401116, 5: 4.395250212044105, 6: 1.7716239316239317, 7: 0.5173205550564041, 8: 0.5887298341286071, 9: 1.755420054200542, 10: 4.0264180264180265, 11: 0.7759808325846062, 12: 0.37315474904587026, 13: 3.6441631504922642, 14: 0.7269921436588104, 15: 1.6083178150217257, 16: 2.109076109076109, 17: 1.3115666919767148}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "print(class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, InputLayer\n",
    "\n",
    "def create_model_with_four_HL(input_shape, num_classes, params={}):\n",
    "    model = tf.keras.Sequential([\n",
    "        InputLayer(input_shape=input_shape),\n",
    "\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Dense(32, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "\n",
    "        Dense(16, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, InputLayer\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def create_deep_dense_model(input_shape, num_classes, params={}):\n",
    "    l2_reg = params.get(\"l2\", 1e-4)\n",
    "    dropout_rate = params.get(\"dropout\", 0.3)\n",
    "\n",
    "    model = Sequential([\n",
    "        InputLayer(input_shape=input_shape),\n",
    "\n",
    "        Dense(512, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "\n",
    "        Dense(256, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "\n",
    "        Dense(128, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "\n",
    "        Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "\n",
    "        Dense(32, activation='relu', kernel_regularizer=regularizers.l2(l2_reg)),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, GlobalMaxPooling1D, Dense\n",
    "\n",
    "def create_simple_bilstm_model(vocab_size, max_len, num_classes):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=64, input_length=max_len),\n",
    "        Bidirectional(LSTM(64, return_sequences=True)),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deep_lstm_model(vocab_size, max_len, num_classes):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=128, input_length=max_len),\n",
    "        Bidirectional(LSTM(128, return_sequences=True)),\n",
    "        Dropout(0.1),\n",
    "        Bidirectional(LSTM(64, return_sequences=True)),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Embedding, \n",
    "    Bidirectional, \n",
    "    LSTM, \n",
    "    Dense, \n",
    "    Dropout, \n",
    "    GlobalAveragePooling1D\n",
    ")\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def create_advanced_lstm_model(vocab_size, max_len, num_classes):\n",
    "    \"\"\"\n",
    "    Creates an advanced, regularized Bidirectional LSTM model for classification.\n",
    "\n",
    "    This model features:\n",
    "    - Two stacked Bidirectional LSTM layers for deeper feature extraction.\n",
    "    - GlobalAveragePooling1D to aggregate features across the sequence.\n",
    "    - Dropout layers and L2 kernel regularization to prevent overfitting.\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): The total size of the vocabulary (number of unique characters/tokens).\n",
    "        max_len (int): The fixed length of input sequences after padding.\n",
    "        num_classes (int): The number of target classes for classification.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: A compiled Keras model ready for training.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # 1. Embedding Layer: Converts token indices into dense vectors.\n",
    "        Embedding(\n",
    "            input_dim=vocab_size, \n",
    "            output_dim=64,       # Dimensionality of the embedding space\n",
    "            input_length=max_len,\n",
    "            mask_zero=True       # Ignores padded zeros in the sequence\n",
    "        ),\n",
    "\n",
    "        # 2. First Bidirectional LSTM Layer\n",
    "        # Processes the sequence of embeddings. return_sequences=True is crucial\n",
    "        # to pass the output of each time step to the next LSTM layer.\n",
    "        Bidirectional(LSTM(64, return_sequences=True)),\n",
    "        \n",
    "        # Regularization: Dropout layer to prevent overfitting on the first LSTM's output.\n",
    "        Dropout(0.3),\n",
    "\n",
    "        # 3. Second Bidirectional LSTM Layer\n",
    "        # This layer receives the sequence from the first LSTM and processes it further.\n",
    "        # return_sequences=True is needed again for the pooling layer to operate on the sequence.\n",
    "        Bidirectional(LSTM(32, return_sequences=True)),\n",
    "\n",
    "        # Regularization: Another dropout layer.\n",
    "        Dropout(0.3),\n",
    "\n",
    "        # 4. Global Average Pooling Layer\n",
    "        # As requested, this averages the features over the entire sequence.\n",
    "        # It's a good alternative to GlobalMaxPooling1D and can be more stable.\n",
    "        GlobalMaxPooling1D(),\n",
    "\n",
    "        # 5. Dense Hidden Layer\n",
    "        # A standard fully-connected layer for learning higher-level feature combinations.\n",
    "        Dense(64, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "\n",
    "        # Regularization: Dropout before the final output layer.\n",
    "        Dropout(0.4),\n",
    "\n",
    "        # 6. Output Layer\n",
    "        # Produces the final probability distribution over the classes.\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Print a summary of the model architecture\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, GlobalMaxPooling1D, Dense, Dropout\n",
    "\n",
    "def create_conv_lstm_model(vocab_size, max_len, num_classes):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=64, input_length=max_len),\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        # Bidirectional(LSTM(128, return_sequences=True)),\n",
    "        # Dropout(0.2),\n",
    "        Bidirectional(LSTM(64, return_sequences=True)),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (Embedding, Conv1D, MaxPooling1D, Bidirectional, \n",
    "                                     LSTM, GlobalMaxPooling1D, Dense, Dropout, LayerNormalization)\n",
    "\n",
    "def create_adv_conv_lstm_model(vocab_size, max_len, num_classes, \n",
    "                           dropout_rate=0.2, use_layer_norm=True):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_len))\n",
    "    \n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'))\n",
    "    if use_layer_norm:\n",
    "        model.add(LayerNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
    "    if use_layer_norm:\n",
    "        model.add(LayerNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "    if use_layer_norm:\n",
    "        model.add(LayerNormalization())\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score,\n",
    "    top_k_accuracy_score\n",
    ")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_classification_model(model, X, y_true, output_path, prefix=\"test\", batch_size=32, top_k=3, label_encoder=None):\n",
    "    # Predict probabilities\n",
    "    y_probs = model.predict(X, batch_size=batch_size, verbose=0)\n",
    "    y_pred = np.argmax(y_probs, axis=1)\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Top-k Accuracy (optional)\n",
    "    top_k_acc = top_k_accuracy_score(y_true, y_probs, k=top_k)\n",
    "\n",
    "    # Classification Report\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).round(2).transpose()\n",
    "    report_df.loc[\"accuracy\"] = acc\n",
    "    report_df.loc[f\"top_{top_k}_accuracy\"] = top_k_acc\n",
    "\n",
    "    # Save Report\n",
    "    report_df.to_csv(f\"{output_path}/cls_report_{prefix}.csv\", index=False)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    if label_encoder is not None:\n",
    "        xticks = yticks = label_encoder.classes_\n",
    "        label_map = dict(enumerate(label_encoder.classes_))\n",
    "    else:\n",
    "        xticks = yticks = np.arange(len(np.unique(y_true)))\n",
    "        label_map = None\n",
    "    print(f\"label map: {label_map}\")\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=xticks, yticklabels=yticks)\n",
    "    plt.title(f\"Confusion Matrix - {prefix}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_path}/confusion_matrix_{prefix}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        f\"top_{top_k}_accuracy\": top_k_acc,\n",
    "        \"classification_report\": report_df,\n",
    "        \"confusion_matrix\": cm\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "def prepare_tokenizer_and_data(df, char_level=True, max_len=40):\n",
    "    # 1. Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(df['SR_Name'])\n",
    "\n",
    "    # 2. Split text and labels early to avoid leakage\n",
    "    X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "        df['name'], y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # 3. Fit tokenizer only on training data\n",
    "    tokenizer = Tokenizer(char_level=char_level, lower=True, filters='')\n",
    "    tokenizer.fit_on_texts(X_train_text)\n",
    "\n",
    "    # 4. Convert to sequences\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
    "    X_test_seq = tokenizer.texts_to_sequences(X_test_text)\n",
    "\n",
    "    # 5. Pad sequences\n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "    X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "\n",
    "    # 6. Compute vocab size\n",
    "    vocab_size = len(tokenizer.word_index) + 1  # +1 for padding (0)\n",
    "\n",
    "    # 7. Compute class weights\n",
    "    class_weights_array = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(y_train),\n",
    "        y=y_train\n",
    "    )\n",
    "    class_weights = dict(enumerate(class_weights_array))\n",
    "\n",
    "    # 8. Print info\n",
    "    print(f\"Vocab size: {vocab_size}\")\n",
    "    print(f\"Max length: {max_len}\")\n",
    "    print(\"X_train shape:\", X_train_pad.shape)\n",
    "    print(\"X_test shape:\", X_test_pad.shape)\n",
    "\n",
    "    return (\n",
    "        X_train_pad, X_test_pad,\n",
    "        y_train, y_test,\n",
    "        vocab_size, max_len,\n",
    "        tokenizer, label_encoder,\n",
    "        class_weights\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def prepare_textvectorization_data(df, max_len=25, char_level=True, ngrams=2):\n",
    "    # 1. Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(df['SR_Name'])\n",
    "\n",
    "    # 2. Train-test split (first!)\n",
    "    X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "        df['name'], y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # 3. Create TextVectorization layer\n",
    "    vectorizer = TextVectorization(\n",
    "        max_tokens=None,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=max_len,\n",
    "        split='character' if char_level else 'whitespace',\n",
    "        ngrams=ngrams\n",
    "    )\n",
    "\n",
    "    # 4. Adapt only on training text\n",
    "    text_ds = tf.data.Dataset.from_tensor_slices(X_train_text).batch(64)\n",
    "    vectorizer.adapt(text_ds)\n",
    "\n",
    "    # 5. Vectorize text\n",
    "    X_train = vectorizer(tf.constant(X_train_text.values)).numpy()\n",
    "    X_test = vectorizer(tf.constant(X_test_text.values)).numpy()\n",
    "\n",
    "    # 6. Vocabulary size\n",
    "    vocab_size = len(vectorizer.get_vocabulary())\n",
    "\n",
    "    print(f\"Vocab size: {vocab_size}\")\n",
    "    print(\"X_train shape:\", X_train.shape)\n",
    "    print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, vocab_size, vectorizer, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 1880\n",
      "Max length: 40\n",
      "X_train shape: (10364, 40)\n",
      "X_test shape: (4443, 40)\n"
     ]
    }
   ],
   "source": [
    "X_train_char, X_test_char, y_train_char, y_test_char, vocab_size_char, max_len_char, tokenizer,label_encoder, class_weights = prepare_tokenizer_and_data(clean_df, char_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 419\n",
      "X_train shape: (10364, 25)\n",
      "X_test shape: (4443, 25)\n"
     ]
    }
   ],
   "source": [
    "X_train_ttok, X_test_ttok, y_train_ttok, y_test_ttok, vocab_size_ttok, vectorizer, label_encoder = prepare_textvectorization_data(clean_df,char_level=True, ngrams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: taung bo\n",
      "Vectorized: [[  5   3  12   4  11   2  63  17  32  22  23  13  36  90 138   0   0   0\n",
      "    0   0   0   0   0   0   0]]\n",
      "Original: minbu\n",
      "Vectorized: [[ 21   7   4  63  12 133  14 280 156   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]]\n",
      "Original: htin zin\n",
      "Vectorized: [[  9   5   7   4   2  89   7   4  79 142  14  16 129 164  14   0   0   0\n",
      "    0   0   0   0   0   0   0]]\n",
      "Original: long wei\n",
      "Vectorized: [[ 24  17   4  11   2  15  10   7 108  33  13  36 103  73  58   0   0   0\n",
      "    0   0   0   0   0   0   0]]\n",
      "Original: ku lar kone\n",
      "Vectorized: [[  6  12   2  24   3  25   2   6  17   4  10 114  72  44  50  28  41  20\n",
      "   57  33  47   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "sample_texts = clean_df['name'].sample(5).tolist()\n",
    "for text in sample_texts:\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Vectorized: {vectorizer([text])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "clean_df['name'].str.len().describe()\n",
    "clean_df['name'].str.len().hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90    14.0\n",
       "0.95    16.0\n",
       "0.99    20.0\n",
       "Name: name, dtype: float64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df['name'].str.len().quantile([0.90, 0.95, 0.99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training time: 26.82 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label map: {0: 'Ayeyarwady', 1: 'Bago (East)', 2: 'Bago (West)', 3: 'Chin', 4: 'Kachin', 5: 'Kayah', 6: 'Kayin', 7: 'Magway', 8: 'Mandalay', 9: 'Mon', 10: 'Nay Pyi Taw', 11: 'Rakhine', 12: 'Sagaing', 13: 'Shan (East)', 14: 'Shan (North)', 15: 'Shan (South)', 16: 'Tanintharyi', 17: 'Yangon'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label map: {0: 'Ayeyarwady', 1: 'Bago (East)', 2: 'Bago (West)', 3: 'Chin', 4: 'Kachin', 5: 'Kayah', 6: 'Kayin', 7: 'Magway', 8: 'Mandalay', 9: 'Mon', 10: 'Nay Pyi Taw', 11: 'Rakhine', 12: 'Sagaing', 13: 'Shan (East)', 14: 'Shan (North)', 15: 'Shan (South)', 16: 'Tanintharyi', 17: 'Yangon'}\n",
      "0.14899842448795858\n",
      "0.14888074102663065\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create and train the model\n",
    "import time\n",
    "model = create_onehot_basic_classification_model(input_shape=[X_train.shape[1]],num_classes=len(df['SR_Name'].unique()), )\n",
    "start_time = time.time()\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stop],verbose=0)\n",
    "end_time = time.time()\n",
    "baseline_training_duration = end_time - start_time\n",
    "print(f\"\\nTraining time: {baseline_training_duration:.2f} seconds\")\n",
    "\n",
    "\n",
    "test_results = evaluate_classification_model(model, X_test, y_test, './data', prefix=\"baeline_test\", label_encoder=le)\n",
    "train_results = evaluate_classification_model(model, X_train, y_train, './data', prefix=\"baeline_train\",label_encoder=le)\n",
    "print(test_results['accuracy'])\n",
    "print(train_results['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training time: 63.17 seconds\n",
      "0.1530497411658789\n",
      "0.21420301042068698\n"
     ]
    }
   ],
   "source": [
    "conv_model_ttok = create_conv_lstm_model(vocab_size_ttok, 25, len(clean_df['SR_Name'].unique()))\n",
    "import time\n",
    "start_time = time.time()\n",
    "history = conv_model_ttok.fit(X_train_ttok, y_train_ttok, epochs=100, batch_size=32, validation_data=(X_test_ttok, y_test_ttok), callbacks=[early_stop],class_weight=class_weights_dict,verbose=0)\n",
    "end_time = time.time()\n",
    "conv_training_duration = end_time - start_time\n",
    "print(f\"\\nTraining time: {conv_training_duration:.2f} seconds\")\n",
    "\n",
    "\n",
    "ttok_conv_test_results = evaluate_classification_model(conv_model_ttok, X_test_ttok, y_test_ttok, './data', prefix=\"ttok_conv_test\")\n",
    "ttok_conv_train_results = evaluate_classification_model(conv_model_ttok, X_train_ttok, y_train_ttok, './data', prefix=\"ttok_conv_train\")\n",
    "print(ttok_conv_test_results['accuracy'])\n",
    "print(ttok_conv_train_results['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training time: 257.81 seconds\n"
     ]
    }
   ],
   "source": [
    "lstm_model = create_simple_bilstm_model(vocab_size_char, max_len_char, len(label_encoder.classes_))\n",
    "import time\n",
    "start_time = time.time()\n",
    "history = lstm_model.fit(X_train_char, y_train_char, epochs=100, batch_size=32, validation_data=(X_test_char, y_test_char),class_weight=class_weights,callbacks=[early_stop], verbose=0)\n",
    "end_time = time.time()\n",
    "lstm_training_duration = end_time - start_time\n",
    "print(f\"\\nTraining time: {lstm_training_duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39278131634819535\n",
      "0.25367270242569184\n"
     ]
    }
   ],
   "source": [
    "simple_lstm_test_results = evaluate_classification_model(lstm_model, X_test_char, y_test_char, './data', prefix=\"simple_lstm_test\")\n",
    "simple_lstm_train_results = evaluate_classification_model(lstm_model, X_train_char, y_train_char, './data', prefix=\"simple_lstm_train\")\n",
    "print(simple_lstm_train_results['accuracy'])\n",
    "print(simple_lstm_test_results['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training time: 535.38 seconds\n"
     ]
    }
   ],
   "source": [
    "ttok_lstm_model = create_simple_bilstm_model(vocab_size_ttok, max_len, len(label_encoder.classes_))\n",
    "import time\n",
    "start_time = time.time()\n",
    "history = ttok_lstm_model.fit(X_train_ttok, y_train_ttok, epochs=100, batch_size=32, validation_data=(X_test_ttok, y_test_ttok),callbacks=[early_stop],class_weight=class_weights_dict, verbose=0)\n",
    "end_time = time.time()\n",
    "lstm_training_duration = end_time - start_time\n",
    "print(f\"\\nTraining time: {lstm_training_duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20447557225828494\n",
      "0.28120653049271543\n"
     ]
    }
   ],
   "source": [
    "ttok_lstm_test_results = evaluate_classification_model(ttok_lstm_model, X_test_ttok, y_test_ttok, './data', prefix=\"ttok_lstm_test\")\n",
    "ttok_lstm_train_results = evaluate_classification_model(ttok_lstm_model, X_train_ttok, y_train_ttok, './data', prefix=\"ttok_lstm_train\")\n",
    "print(ttok_lstm_test_results['accuracy'])\n",
    "print(ttok_lstm_train_results['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training time: 939.63 seconds\n"
     ]
    }
   ],
   "source": [
    "deep_lstm_model = create_deep_lstm_model(vocab_size_ttok, 50, len(clean_df['SR_Name'].unique()))\n",
    "import time\n",
    "start_time = time.time()\n",
    "history = deep_lstm_model.fit(X_train_ttok, y_train_ttok, epochs=100, batch_size=32, validation_data=(X_test_ttok, y_test_ttok),callbacks=[early_stop],class_weight=class_weights_dict, verbose=0)\n",
    "end_time = time.time()\n",
    "lstm_training_duration = end_time - start_time\n",
    "print(f\"\\nTraining time: {lstm_training_duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1959958836186734"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttok_deep_lstm_test_results = evaluate_classification_model(deep_lstm_model, X_test_ttok, y_test_ttok, './data', prefix=\"ttok_deep_lstm_test\")\n",
    "ttok_deep_lstm_train_results = evaluate_classification_model(deep_lstm_model, X_train_ttok, y_train_ttok, './data', prefix=\"ttok_deep_lstm_train\")\n",
    "ttok_deep_lstm_test_results['accuracy']\n",
    "ttok_deep_lstm_train_results['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13075747653350797\n",
      "0.1959958836186734\n"
     ]
    }
   ],
   "source": [
    "print(ttok_deep_lstm_test_results['accuracy'])\n",
    "print(ttok_deep_lstm_train_results['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 47\n",
      "sequences: 19513\n",
      "X_train shape: (13659, 20)\n",
      "X_test shape: (5854, 20)\n",
      "y_train shape: (13659,)\n",
      "y_test shape: (5854,)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 20, 64)            3008      \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 20, 128)           66048     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 20, 128)           0         \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirecti  (None, 20, 64)            41216     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 20, 64)            0         \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 64)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 18)                1170      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 115602 (451.57 KB)\n",
      "Trainable params: 115602 (451.57 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "\n",
      "Training time: 849.98 seconds\n"
     ]
    }
   ],
   "source": [
    "X_train_char, X_test_char, y_train_char, y_test_char, vocab_size_char = prepare_tokenizer_and_data(new_df, char_level=True)\n",
    "adv_lstm_model = create_advanced_lstm_model(vocab_size_char, 20, len(new_df['SR_Name'].unique()))\n",
    "import time\n",
    "start_time = time.time()\n",
    "history = adv_lstm_model.fit(X_train_char, y_train_char, epochs=50, batch_size=32, validation_data=(X_test_char, y_test_char), verbose=0)\n",
    "end_time = time.time()\n",
    "lstm_training_duration = end_time - start_time\n",
    "print(f\"\\nTraining time: {lstm_training_duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "adv_lstm_test_results = evaluate_classification_model(adv_lstm_model, X_test_char, y_test_char, './data', prefix=\"adv_lstm_test\")\n",
    "adv_lstm_train_results = evaluate_classification_model(adv_lstm_model, X_train_char, y_train_char, './data', prefix=\"adv_lstm_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training time: 475.58 seconds\n"
     ]
    }
   ],
   "source": [
    "lstm_model = create_simple_bilstm_model(vocab_size_char, 20, len(new_df['SR_Name'].unique()))\n",
    "import time\n",
    "start_time = time.time()\n",
    "history = lstm_model.fit(X_train_char, y_train_char, epochs=50, batch_size=32, validation_data=(X_test_char, y_test_char), verbose=0)\n",
    "end_time = time.time()\n",
    "lstm_training_duration = end_time - start_time\n",
    "print(f\"\\nTraining time: {lstm_training_duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_test_results = evaluate_classification_model(lstm_model, X_test_char, y_test_char, './data', prefix=\"lstm_test\")\n",
    "lstm_train_results = evaluate_classification_model(lstm_model, X_train_char, y_train_char, './data', prefix=\"lstm_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training time: 108.57 seconds\n"
     ]
    }
   ],
   "source": [
    "#print(tokenizer.word_index)\n",
    "import time\n",
    "fourhl_model = create_model_with_four_HL(input_shape=[X_train.shape[1]],num_classes=len(df['SR_Name'].unique()))\n",
    "start_time = time.time()\n",
    "history = fourhl_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test),callbacks=[early_stop], verbose=0)\n",
    "end_time = time.time()\n",
    "baseline_training_duration = end_time - start_time\n",
    "print(f\"\\nTraining time: {baseline_training_duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourHL_test_results = evaluate_classification_model(fourhl_model, X_test, y_test, './data', prefix=\"4HL_test\")\n",
    "fourHL_train_results = evaluate_classification_model(fourhl_model, X_train, y_train, './data', prefix=\"4HL_train\")\n",
    "print(fourHL_test_results['accuracy'])\n",
    "print(fourHL_train_results['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training time: 403.05 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "deepdense_model = create_deep_dense_model(input_shape=[X_train.shape[1]],num_classes=len(df['SR_Name'].unique()))\n",
    "start_time = time.time()\n",
    "history = deepdense_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test),callbacks=[early_stop], verbose=0)\n",
    "end_time = time.time()\n",
    "baseline_training_duration = end_time - start_time\n",
    "print(f\"\\nTraining time: {baseline_training_duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\nuwai\\Anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2210454390160574\n",
      "0.4525953583717695\n"
     ]
    }
   ],
   "source": [
    "deepdense_test_results = evaluate_classification_model(deepdense_model, X_test, y_test, './data', prefix=\"deepdense_test\")\n",
    "deepdense_train_results = evaluate_classification_model(deepdense_model, X_train, y_train, './data', prefix=\"deepdense_train\")\n",
    "print(deepdense_test_results['accuracy'])\n",
    "print(deepdense_train_results['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 5: Evaluate the model\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# y_pred = model.predict(X_test, batch_size=32, verbose=0)\n",
    "# y_pred = y_pred.argmax(axis=1)\n",
    "# report = classification_report(y_test, y_pred, output_dict=True)\n",
    "# report_df = pd.DataFrame(report).round(2).transpose()\n",
    "# report_df.to_csv('./data/cls_report_test.csv', index=False)\n",
    "\n",
    "# y_pred = model.predict(X_train, batch_size=32, verbose=0)\n",
    "# y_pred = y_pred.argmax(axis=1)\n",
    "# report = classification_report(y_train, y_pred, output_dict=True)\n",
    "# report_df = pd.DataFrame(report).round(2).transpose()\n",
    "# report_df.to_csv('./data/cls_report_train.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
